,question,contexts,answer
0,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
1,What is the title of the paper on Retrieval Augmented Generation mentioned in the context?,"['the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']", FLARE: Forward-Looking Active REtrieval Augmented Generation
2,What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?,"['the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', 'retrieved documents from C using the retriever it-\nself (Xiong et al., 2020). The loss function l() can\nbe any standard ranking loss such as cross entropy. A\nZeroDR model is trained on qs and documents ds ∈ Cs\nfrom a source task, often web search, and tested on tar-\nget tasks qt and Ct; supervision signals are only present\nfrom the source.\nMixture-of-Memory Augmentation. The key idea\nof (document-based) retrieval augmented language mod-\nels is to enrich the representation g(q) with additional\ncontextual input for the model, i.e., augmentation doc-\numents da retrieved from an external memory M. In-\nstead of using a single document corpus, MoMA uses\nmultiple corpora to provide richer and more diverse ex-\nternal resources for augmentation. For example, M\ncan be composed by the source corpus Cs, a general\nencyclopedia, a domain speciﬁc knowledge graph, etc.\nThen we can retrieve the augmentation documents Da :\nDa = ANNM\nf a(x,◦); M = {C1, ..., CM}.\n(4)']"," The title of the paper on Retrieval Augmented Generation mentioned in the context information is """
3,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
4,What is the task of the Retrieval Augmented Generation (RAG) model according to the context information?,"['boost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various', 'may beneﬁt the response generation, if xr (or yr)\nis similar (or relevant) to the input x. It is worth\nnoting that xr = ∅ when unsupervised retrieval\nsources are used. In general, the retrieval mem-\nory can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (§2.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n§2.3 we divided them into three categories: sparse-\nvector retrieval, dense-vector retrieval, and training-\nbased retrieval. Finally, how to integrate the re-\ntrieval memory to the generation model is also sig-\nniﬁcant, we also introduce some popular integra-\ntion approaches in §2.4.\n2.2\nRetrieval Sources\nTraining Corpus\nMost previous studies search\nthe external memory from its training corpus (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'throughout the generation process, and are appli-\ncable to a variety of long-form generation tasks?\nWe consider a new paradigm, active retrieval aug-\nmented generation. Our hypothesis regarding when\nto retrieve is that LMs should retrieve information\nonly when they lack the required knowledge to\navoid unnecessary or inappropriate retrieval that\noccurs in passive retrieval-augmented LMs (Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\net al., 2023; Trivedi et al., 2022). Given the obser-\nvation that large LMs tend to be well-calibrated and\nlow probability/conﬁdence often indicates a lack\nof knowledge (Jiang et al., 2021; Kadavath et al.,\n2022), we adopt an active retrieval strategy that\nonly retrieves when LMs generate low-probability\ntokens. When deciding what to retrieve, we argue\nthat it is important to consider what LMs intend to\ngenerate in the future, as the goal of active retrieval\nis to beneﬁt future generations. Therefore, we pro-']", The task of the Retrieval Augmented Generation (RAG) model is to generate meaningful
5,What advantages does RAG have over the DPR QA system in terms of generating answers?,"['and compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.\n6.1\nComparison with Baselines\nOverall results.\nThe overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks and datasets, multihop QA\nshows the most signiﬁcant improvement. This is\nlargely due to the task’s clear deﬁnition and speciﬁc\nobjective of producing the ﬁnal answer through a\n2-hop reasoning process, which makes it easier\nfor LMs to generate on-topic output. In contrast,\nASQA and WikiAsp are less clearly deﬁned and\nmore open-ended, which increases the difﬁculty of\n6To avoid leaking, we exclude several Wikipedia-related', '(Fan et al., 2019), and found that FLARE did not\nprovide signiﬁcant gains. Wizard of Wikipedia is\na knowledge-intensive dialogue generation dataset\nwhere the output is relatively short (∼20 tokens\non average) so retrieving multiple disparate pieces\nof information might not be necessary. ELI5 (Fan\net al., 2019) is a long-form QA dataset requiring\nin-depth answers to open-ended questions. Due to\nissues mentioned in Krishna et al. (2021) such as\ndifﬁculties of grounding generation in retrieval and\nevaluation, both single-time retrieval and FLARE\ndid not provide signiﬁcant gains over not using\nretrieval. From an engineering perspective, inter-\nleaving generation with retrieval with a naive im-\nplementation increases both overheads and the cost\nof generation. The LM needs to be activated mul-\ntiple times (once for each retrieval) and a caching-\nfree implementation will also require recomputing\nthe previous activation each time after a retrieval.', 'lected based on the development set and listed in\nTable 8. FLARE refers to FLAREdirect if not specif-\nically stated. For previous-window approaches,\nwe follow Ram et al. (2023) to use a window size\nl = 16 in our experiments.\n5.1\nMultihop QA\nDataset\nThe goal of multihop QA is to answer\ncomplex questions through a process of informa-\ntion retrieval and reasoning (Yang et al., 2018; Ho\net al., 2020). For instance, to answer “Why did\nthe founder of Versus die?”, we must ﬁrst identify\nwho founded Versus and subsequently determine\nthe cause of their death. Multihop QA also uniﬁes\ninto long-form generation when solved with the\nstate-of-the-art CoT methods (Wei et al., 2022).\nWe use 2WikiMultihopQA (Ho et al., 2020)\nwhich contains 2-hop complex questions sourced\nfrom Wikipedia articles that require composition,\ncomparison, or inference.\nSettings\nWe follow Wang et al. (2022) to gen-\nerate both the chain-of-thought reasoning process\nand the ﬁnal answer. For the above case, the output', 'LM (Ans-Deletion)\n(b) Answer-deletion test.\nFigure 6: Analysis of LM-preferred documents. (a)\nshows the overlaps of positive document sets, where\nused LMs are Flan-T5 series. (b) presents the answer-\ndeletion experiments on the MSMARCO QA dataset.\nThe retriever is initialized from ANCE.\n(Dh+) and LMs (Top-KSa\ni ,Da) are quite low (near\n13%), demonstrating their distinct tendencies in\nselecting valuable documents. On the contrary, the\noverlaps between different LMs are relatively high\n(over 55%). This evidence provides a strong ratio-\nnale for the generalization ability of AAR since\nLMs with different sizes tend to annotate simi-\nlar positive documents. Furthermore, LMs whose\nsizes are closer generally possess higher overlaps.\nThis implies a better generalization ability of the\nAAR to the LMs whose capacity is near the source\nLM. The findings further validate the results illus-\ntrated in Figure 4b.\nTo give an in-depth analysis of how human-']", RAG has several advantages over the DPR QA system in terms of generating answers. Firstly
6,"What are some potential downsides of using RAG, a language model based on Wikipedia, in various scenarios?","['Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\nAssociation for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,', 'On MMLU, using MS MARCO as the retrieval\ncorpus improves the LM more compared to KILT-\nWikipedia. We hypothesize that the retriever has\nbeen trained with MS MARCO corpus and thus\nholds better retrieval performance on it.\nSettings\nMethods\nMMLU\nPopQA\nAll\nAll\nFew-shot\nOPT (Zhang et al., 2022)\n26.0\n12.3\nGPT-neo (Black et al., 2021)\n28.7\n11.3\nZero-shot\nOPT\n22.7\n12.0\nGPT-neo\n25.3\n9.9\nOPT GenRead\n22.3\n12.2\nGPT-neo GenRead\n24.4\n11.9\nOPT w/ AARContriever (Ours)\n23.2\n29.1\nGPT-neo w/ AARContriever (Ours)\n25.2\n27.8\nOPT w/ AARANCE (Ours)\n23.7\n32.9\nGPT-neo w/ AARANCE (Ours)\n26.6\n30.1\nTable 4: Results of OPT and GPT-neo. We use their\n1.3B version. The score marked as bold means the best\nperformance in the zero-shot setting.\nOn PopQA, model performance will drop by\nlarge margins if we use MS MARCO as the re-\ntrieval corpus instead of KILT-Wikipedia. The pri-\nmary reason is that the PopQA dataset is sampled\nfrom Wikidata and designed for long-tail questions.', 'leverage the augmentation information from multiple\ncorpora during training and testing. We hope our ﬁnd-\nings and illustrations can inspire more future research in\nbetter augmenting language models, to provide other al-\nternatives to achieve generalization ability beyond solely\nrelying on model scale.\nLimitations\nAlthough MoMA (T5-ANCE) and MoMA (COCO)\nachieve strong zero-shot performances, we mainly ver-\nify their efﬁcacy from the empirical performances\non BEIR tasks, where the target corpora, Wiki and\nMARCO serve as readily available retrieval sources.\nIn a real-world scenario, the grounding corpora usually\nneed to be customized according to query domains and\nuser needs. Thus, how to choose effective grounding\ncorpora and efﬁciently evaluate their relative contribu-\ntion remain an open problem. These analyses will go\nbeyond our empirical settings and reveal a wider appli-\ncation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used', 'narayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nCoRR, abs/2204.02311.\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\nAlessandro Moschitti. 2021.\nWikisum:\nCoher-\nent summarization dataset for efﬁcient human-\nevaluation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 2: Short Papers), Virtual Event, August 1-\n6, 2021, pages 212–219. Association for Computa-\ntional Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational']","

There are several potential downsides of using RAG, a language model based on Wikipedia"
7,What is the file name of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', 'Zichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search']"," The file name of the paper on Retrieval Augmented Generation is ""FLARE: For"
8,What is the accuracy of RAG models in generating correct answers even when the correct answer is not in any retrieved document?,"['LM (Ans-Deletion)\n(b) Answer-deletion test.\nFigure 6: Analysis of LM-preferred documents. (a)\nshows the overlaps of positive document sets, where\nused LMs are Flan-T5 series. (b) presents the answer-\ndeletion experiments on the MSMARCO QA dataset.\nThe retriever is initialized from ANCE.\n(Dh+) and LMs (Top-KSa\ni ,Da) are quite low (near\n13%), demonstrating their distinct tendencies in\nselecting valuable documents. On the contrary, the\noverlaps between different LMs are relatively high\n(over 55%). This evidence provides a strong ratio-\nnale for the generalization ability of AAR since\nLMs with different sizes tend to annotate simi-\nlar positive documents. Furthermore, LMs whose\nsizes are closer generally possess higher overlaps.\nThis implies a better generalization ability of the\nAAR to the LMs whose capacity is near the source\nLM. The findings further validate the results illus-\ntrated in Figure 4b.\nTo give an in-depth analysis of how human-', 'AAR to the LMs whose capacity is near the source\nLM. The findings further validate the results illus-\ntrated in Figure 4b.\nTo give an in-depth analysis of how human-\npreferred and LM-preferred documents differ, we\nshow two representative cases sampled from the\nMSMARCO QA in Table 2. We observe that the\nhuman-preferred document can always present the\ngold answer at the beginning of the text, while the\nLM-preferred document may not contain the ex-\nact answer. However, an LM-preferred document\n250M\n780M\n3B\n175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75\nMMLU Accuracy\nTART\nAARContriever (MSMARCO QA)\nAARContriever (KILT)\nAARANCE (MSMARCO QA)\nAARANCE (KILT)\nFigure 7:\nComparison between single-task (MS-\nMARCO QA) and multi-task (KILT) trained AAR.\nTART (Asai et al., 2022) is a multi-task instruction-\nfinetuned retriever that has not been finetuned with LM-\npreferred signals.\ncan (1) deliver a new perspective to answer the\ngiven question, e.g., “the cruise line’s responsibil-', 'that our model consistently outperforms all the other baselines, demonstrating the superior performance.\nHowever, it is worth noting that the perplexity of the OPT-125M model does not consistently surpass that\nof the HybridRAG w/o FT model.\nF\nMore examples from human evaluation\nTable 8 shows another working example for HybridRAG models and Table 9 shows an example of a\nfailing case for both OPT-125M and OPT-350M HybridRAG models. The OPT-125M model generates\na repetitive and factually incorrect text, while the OPT-350M model generates a text that is factually\nincorrect. GPT3 Davinci, however, can still use the same retrieved memories to provide a factual and useful\ncompletion for this prompt. The memories are bullet points generated from several document chunks;\nPPL\nGLEU\nBLEU-4\nROUGE-1\nROUGE-L\nMETEOR\nBERTScore\nEnron\nEmails\nGPT3 zero-shot\n106.9\n12.3\n10.4\n26.1\n23.3\n21.6\n83.6\nVanilla OPT\n5.4\n8.7\n6.5\n21.1\n18.6\n17.1\n80.2\nRAG\n3.4\n13.6\n12.2\n26.8\n24.0\n22.9\n81.2\nHybridRAG w/o FT\n2.9\n19.9\n19.4', 'the open-domain setting by removing the associ-\nated references and instead gathering information\nfrom the open web. For the above case, the output\nwe aim to generate is “# Academics. In 2008, 91%\nof the school’s seniors received their high school\ndiploma... # History. The class of 2008 was the\n100th class in the school’s history.” where # is used\nto indicate aspects. We manually annotate 4 exem-\nplars (Prompt C.8), and use the Bing search engine\nto retrieve 5 documents from the open web.6\nEvaluation\nWe compare system outputs with the\ngold summary using ROUGE, named entity-based\nF1, and UniEval (Zhong et al., 2022) which mea-\nsures factual consistency based on prediction proba-\nbility of a ﬁne-tuned T5 model (Raffel et al., 2020).\n6\nExperimental Results\nWe ﬁrst report overall results across 4 tasks/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.']", The provided context does not mention the accuracy of RAG models in generating correct answers even when the correct
9,Question: What is an example of how parametric and non-parametric memories work together in BART?,"['that our model consistently outperforms all the other baselines, demonstrating the superior performance.\nHowever, it is worth noting that the perplexity of the OPT-125M model does not consistently surpass that\nof the HybridRAG w/o FT model.\nF\nMore examples from human evaluation\nTable 8 shows another working example for HybridRAG models and Table 9 shows an example of a\nfailing case for both OPT-125M and OPT-350M HybridRAG models. The OPT-125M model generates\na repetitive and factually incorrect text, while the OPT-350M model generates a text that is factually\nincorrect. GPT3 Davinci, however, can still use the same retrieved memories to provide a factual and useful\ncompletion for this prompt. The memories are bullet points generated from several document chunks;\nPPL\nGLEU\nBLEU-4\nROUGE-1\nROUGE-L\nMETEOR\nBERTScore\nEnron\nEmails\nGPT3 zero-shot\n106.9\n12.3\n10.4\n26.1\n23.3\n21.6\n83.6\nVanilla OPT\n5.4\n8.7\n6.5\n21.1\n18.6\n17.1\n80.2\nRAG\n3.4\n13.6\n12.2\n26.8\n24.0\n22.9\n81.2\nHybridRAG w/o FT\n2.9\n19.9\n19.4', 'Sherlock moved to Belsize Park in\nVanilla OPT-350M\n) was an English cricketer. He played first-class cricket for Surrey in the 1960s and 1970s.\nReferences\nExternal links\nHybridRAG OPT-350M\n) was a British comedian and actor. He was born in London, the son of a British diplomat.\nHe was educated at Emmanuel College, Cambridge, where he studied medicine. He also\njoined the Cambridge Footlights, where\nTable 5: Completions of the prompt by different models, showcasing a working case for HybridRAG OPT-125M;\nthis is also a working case for HybridRAG OPT-350M if we only consider the first sentence.\nindicate that the HybridRAG OPT-125M has a bet-\nter balance between the retrieval and generation\nmechanisms.\nHowever, we also observe cases where the Hy-\nbridRAG model fails (refer to Appendix F). Im-\nproving the memory generator by reducing dupli-\ncate information, and enhancing the reasoning abil-\nities of the client model or encouraging it to stick to', 'boost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various', 'LMs, including Flan-T5 (Chung et al., 2022), In-\nstructGPT (Ouyang et al., 2022), GAL (Taylor\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\nare prompted by a natural language instruction that\ndescribes the desired task and question. Adaptive\nretrieval (Mallen et al., 2022) selectively utilizes\nnon-parametric memory (retrieval augmentation)\nand parametric memory (the knowledge obtained\nfrom pre-training) based on questions’ popularity.\nIn our main experiment, we select the optimal com-\nbination in their paper, which consists of Contriever\nas the non-parametric memory and GenRead (Yu\net al., 2023) as the parametric memory.\nFew-shot Setting. We also include the results of\nprevious few-shot models for reference. Flan-T5,\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\nand OPT-IML-Max adopt few-shot demonstrations,\nwhich provide the LMs with a limited number of\ntask examples. This enables the models to gener-\nalize from these examples and generate accurate']"," In the context of BART, parametric memory refers to the knowledge obtained from pre-training,"
10,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
11,What is the purpose of using multiple answer annotations in open-domain QA?,"['lected based on the development set and listed in\nTable 8. FLARE refers to FLAREdirect if not specif-\nically stated. For previous-window approaches,\nwe follow Ram et al. (2023) to use a window size\nl = 16 in our experiments.\n5.1\nMultihop QA\nDataset\nThe goal of multihop QA is to answer\ncomplex questions through a process of informa-\ntion retrieval and reasoning (Yang et al., 2018; Ho\net al., 2020). For instance, to answer “Why did\nthe founder of Versus die?”, we must ﬁrst identify\nwho founded Versus and subsequently determine\nthe cause of their death. Multihop QA also uniﬁes\ninto long-form generation when solved with the\nstate-of-the-art CoT methods (Wei et al., 2022).\nWe use 2WikiMultihopQA (Ho et al., 2020)\nwhich contains 2-hop complex questions sourced\nfrom Wikipedia articles that require composition,\ncomparison, or inference.\nSettings\nWe follow Wang et al. (2022) to gen-\nerate both the chain-of-thought reasoning process\nand the ﬁnal answer. For the above case, the output', 'and compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.\n6.1\nComparison with Baselines\nOverall results.\nThe overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks and datasets, multihop QA\nshows the most signiﬁcant improvement. This is\nlargely due to the task’s clear deﬁnition and speciﬁc\nobjective of producing the ﬁnal answer through a\n2-hop reasoning process, which makes it easier\nfor LMs to generate on-topic output. In contrast,\nASQA and WikiAsp are less clearly deﬁned and\nmore open-ended, which increases the difﬁculty of\n6To avoid leaking, we exclude several Wikipedia-related', 'ing. In North American Association for Computa-\ntional Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: a benchmark for question answer-\ning research.\nTrans. Assoc. Comput. Linguistics,\n7:452–466.\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\njape, Christopher D. Manning, and Kyoung-Gu Woo.\n2021. You only need one model for open-domain\nquestion answering. CoRR, abs/2112.07381.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks.\nIn Advances in', 'The following question “Where do the Philadelphia\nEagles play their home games?” could be asking\nabout the city, sports complex, or stadium of their\nhome games. We use ASQA (Stelmakh et al., 2022)\nas our testbed where inputs are ambiguous ques-\ntions with multiple interpretations, and outputs are\ncomprehensive answers covering all.\nSetting\nTo answer ambiguous questions, systems\nmust ﬁrst identify possible interpretations and then\nprovide answers for each of them, which for the\nabove case is “We need to consider the different\npossible locations or venues that could be con-\nsidered the home ﬁeld of the Philadelphia Eagles.\nThese include the city, the sports complex, or the\nstadium. Therefore, this question has 3 interpreta-\ntions and the answers are: (1) The city is Philadel-\nphia. (2) The sports complex is the South Philadel-\nphia Sports Complex. (3) The stadium is the Lin-\ncoln Financial Field stadium.” We found that in\nmany cases, it is challenging even for humans to']", Multiple answer annotations are used in open-domain QA to provide a comprehensive answer that covers
12,Question: Which novel by Ernest Hemingway is based on his wartime experiences?,"['January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\nPrompt C.5: exemplars of ASQA (cont.)\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\nQuestion: What was roy orbison’s ﬁrst number one hit?\nAnswer: This question is ambiguous in terms of which speciﬁc chart or region is being referred to. In order to ﬁgure out its\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\nwas Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s ﬁrst number one hit in\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the ﬁrst number one hit for Roy Orbison\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the ﬁrst number one hit for Roy Orbison in', 'Answer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\n1963. So the answer is August 25, 1963.\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\nAnswer (with step-by-step): The ﬁlm Twenty Plus Two was directed by Joseph M. Newman.\nThe Gal Who Took\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\nNow, combine the aforementioned two skills.\nFirst, write out the reasoning steps, then draw the conclusion,', 'left. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\ndestroyed fort was replaced with the present day Castillo de San Marcos.\n# Background\nWar had already been unofﬁcially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\nEngland, he struck ﬁrst at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before ﬁnally visiting and', 'music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\nquestion has 2 interpretations: (1) What was Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What\nwas Roy Orbison’s ﬁrst number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\nwas the ﬁrst number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\nthe ﬁrst number one hit for Roy Orbison in the UK and Ireland.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\ntions and answer them one by one.\nQuestion: What is the criminal’s name in the breakfast club?\nHint: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\nname.\nAnswer: In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s']", The novel “For Whom the Bell Tolls” is based on Ernest Hemingway’
13,What advantage do non-parametric memory models like RAG have over parametric-only models like T5 or BART?,"['lions of parameters are able to capture a signif-\nicant amount of human knowledge, leading to\nconsistent improvements on various downstream\ntasks (Brown et al., 2020; Kaplan et al., 2020;\nRoberts et al., 2020). However, the undeniable\ndrawback of large LMs lies in their high compu-\ntational cost, which negatively impacts their effi-\nciency (Strubell et al., 2019; Bender et al., 2021).\nFurthermore, the knowledge memorized from pre-\ntraining and the implicit reasoning process of LMs\ncan be inaccurate and intractable sometimes, hin-\ndering their applications on knowledge-intensive\ntasks (Guu et al., 2020; Lewis et al., 2020; Mallen\net al., 2022; Wei et al., 2022).\nFlan-T5Base\n(250M)\nFlan-T5Large\n(780M)\nFlan-T5XL\n(3B)\nInstructGPT\n(175B)\n# Parameters\n35\n40\n45\n50\n55\n60\n65\nMMLU Accuracy\nStandalone LM\nLM w/ Few-Shot Prompting\nLM w/ Adaptive Retrieval\nLM w/ AAR (Ours)\nFigure 1: Performance of LM w/ AAR (Ours).\nInstead of leveraging the knowledge and rea-', 'LMs, including Flan-T5 (Chung et al., 2022), In-\nstructGPT (Ouyang et al., 2022), GAL (Taylor\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\nare prompted by a natural language instruction that\ndescribes the desired task and question. Adaptive\nretrieval (Mallen et al., 2022) selectively utilizes\nnon-parametric memory (retrieval augmentation)\nand parametric memory (the knowledge obtained\nfrom pre-training) based on questions’ popularity.\nIn our main experiment, we select the optimal com-\nbination in their paper, which consists of Contriever\nas the non-parametric memory and GenRead (Yu\net al., 2023) as the parametric memory.\nFew-shot Setting. We also include the results of\nprevious few-shot models for reference. Flan-T5,\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\nand OPT-IML-Max adopt few-shot demonstrations,\nwhich provide the LMs with a limited number of\ntask examples. This enables the models to gener-\nalize from these examples and generate accurate', 'on the four Pile datasets are presented in Table 4.\nConsistent with the findings on the Wikitext-103\ndataset, our model demonstrates superior perfor-\nmance compared to the baseline models across all\nfour datasets. It is important to note that we did\nnot finetune the client model specifically on the\nPile datasets, further highlighting the model’s gen-\neralization capabilities. Lastly, we have also ob-\nserved a significantly high perplexity when using\nthe GPT3 Davinci model. This is due to substan-\ntial variance in probability between the model’s\npredictions (generated by GPT3 zero-shot) and the\nground truth labels (generated by GPT3 with aug-\nmented memory).\n4.2.2\nInference Latency\nWe performed a latency evaluation for both the\nOPT-125M and OPT-350M models on the two hard-\nware setups, as described in Section 4.1.2. Figure\n5a illustrates that the OPT-125M model exhibits a\n49.3% faster inference time compared to the OPT-\n350M model. This finding emphasizes that the size', 'model for generation. (iii) HybridRAG without fine-\ntuning (HybridRAG w/o FT): To assess the efficacy\nof our instruction-tuned client model, we examine\na HybridRAG model without applying finetuning\nto the cilent model for text prediction. (iv) GPT3\nzero-shot: We use the GPT3 Davinci model in a\nzero-shot manner for text completion. However,\nit’s important to note that the GPT3 model cannot\nbe deployed on client devices for real-time compo-\nsition assistance.\nWhen evaluating the baseline models, we ensure\na fair comparison by regenerating reference labels\nusing the GPT3 model, based on the memory used\n3https://en.wikipedia.org/wiki/Perplexity\nby each baseline. Specifically, for the Vanilla OPT\nbaseline, reference labels are generated with GPT3\nwithout additional memory. For RAG, reference\nlabels are generated by GPT3 with full text. In the\ncase of GPT3-zeroshot baseline, since there is no\nideal reference label for comparison, we used the\nsame label as our HybridRAG approach.\n4.2']", Non-parametric memory models like RAG have the advantage of being able to store and retrieve information
14,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
15,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
16,What training setup details are mentioned in the paper on Retrieval Augmented Generation?,"['Zichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', 'margins with fewer training FLOPs.\n5.2\nAblation Study\nIn this experiment, we conduct the ablation study of\naugmentation-adapted training and analyze model\nbehaviors during the training process.\nFigure 4a illustrates that augmentation-adapted\ntraining can bring additional improvements com-\npared to the pre-trained retrievers.\nIn general,\nANCE benefits more from augmentation-adapted\ntraining than Contriever. This may be due to the\nfact that Contriever has been already intensively\npre-trained on massive data augmentations as well\nas MS MARCO whereas ANCE is trained only on\nMS MARCO. We provide exact numbers in Table 7\nand PopQA results in Figure 8, which yield similar\nobservations as MMLU.\nIn Figure 4b, we compare retrievers trained with\ndifferent positive documents, including human-\npreferred documents annotated by search users (the\nblue bar), LM-preferred documents obtained by\nthe source LM (the orange bar), and their combi-\nnations (the green bar and the red bar). Since the', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'a memory-augmented client model (client), a re-\ntriever model (cloud), a memory generator (cloud).\nFigure 1 illustrates the model architecture. The\naugmentation coordinator monitors the writing con-\ntext and determines when to request an augmented\nmemory from the cloud. The retriever model on\nthe cloud server then searches the retrieval corpus\nto find relevant data. Subsequently, the memory\ngenerator employs the GPT3 model to construct\nan augmented memory that includes all essential\ninformation from the retrieved data, optimizing\nits usefulness. Finally, the augmented memory is\ntransmitted to the client and seamlessly integrated\ninto the client model, thereby enhancing its overall\nperformance.\n3.2\nAugmentation Coordinator\nThe augmentation coordinator component is re-\nsponsible for managing the augmented memory\nM by monitoring changes to the writing context.\nThe entire process of the augmentation coordinator\nis depicted in Figure 2. To determine whether a']"," The paper on Retrieval Augmented Generation mentions the following training setup details:

"
17,What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?,"['the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', 'retrieved documents from C using the retriever it-\nself (Xiong et al., 2020). The loss function l() can\nbe any standard ranking loss such as cross entropy. A\nZeroDR model is trained on qs and documents ds ∈ Cs\nfrom a source task, often web search, and tested on tar-\nget tasks qt and Ct; supervision signals are only present\nfrom the source.\nMixture-of-Memory Augmentation. The key idea\nof (document-based) retrieval augmented language mod-\nels is to enrich the representation g(q) with additional\ncontextual input for the model, i.e., augmentation doc-\numents da retrieved from an external memory M. In-\nstead of using a single document corpus, MoMA uses\nmultiple corpora to provide richer and more diverse ex-\nternal resources for augmentation. For example, M\ncan be composed by the source corpus Cs, a general\nencyclopedia, a domain speciﬁc knowledge graph, etc.\nThen we can retrieve the augmentation documents Da :\nDa = ANNM\nf a(x,◦); M = {C1, ..., CM}.\n(4)']"," The title of the paper on Retrieval Augmented Generation mentioned in the context information is """
18,What is the title of the paper mentioned in the context information?,"['FEVER\n(Thorne\net\nal.,\n2018),\nand\nClimate-\nFEVER (Diggelmann et al., 2020)\nWe list the statistics of the BEIR benchmark in Table 7.\nAugmenting Corpora\nCorpus size We ﬁrst introduce\nmore details on how we preprocessed the Medical Sub-\nject Headings (MeSH) Database. We select text in-\nformation from the Qualiﬁer Record Set and Descrip-\ntor Record Set. Each set contains multiple <Concept>\nelements, which is composed of three sub-elecments,\ni.e., <ConceptName>, <ScopeNote> and <TermList>.\nAmong the sub-elecments, <ScopeNote> is the major\ntextual information source, which is usually a short de-\nscription to a medical term or phenomenon. We directly\nconsider each <ScopeNote> as a document entry and\nconcatenate it with corresponding <ConceptName>.\nWe list the statistics of the augmenting corpora in\nTable 8.\nA.2\nBaselines\nWe use the baselines from the current BEIR leader-\nboard (Thakur et al., 2021b) and recent papers. These\nbaselines can be divided into four groups: dense re-', 'Zichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', 'dataset for medical information retrieval. In Euro-\npean Conference on Information Retrieval, pages\n716–722. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nClaudio Carpineto and Giovanni Romano. 2012. A\nsurvey of automatic query expansion in information\nretrieval. Acm Computing Surveys (CSUR), 44(1):1–\n50.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1870–1879.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel Weld. 2020.\nSPECTER:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-informed transformers.\nIn Proceedings\nof the 58th Annual Meeting of the Association for', 'ments. For datasets that mainly rely on knowledge\nfrom Wikipedia, we use the Wikipedia dump from\nKarpukhin et al. (2020) where articles are divided\ninto 100-token passages as the document corpus\nand employ BM25 (Robertson and Zaragoza, 2009)\nas the retriever. For datasets that rely on knowledge\nfrom the open web, we use the Bing search engine\nas our retriever.4\nRetrieved document formatting.\nMultiple re-\ntrieved documents are linearized according to their\nranking and then added to the beginning of the user\ninput using the following format:\nPrompt 3.3: document formatting\nSearch results:\n[1] Document 1\n[2] Document 2\n...\nThe user input x\nEfﬁciency\nAs shown later in subsection 6.2, on\naverage retrieval is triggered for 30% ∼ 60% of\nsentences depending on downstream tasks.\nIn\ncomparision, KNN-LM (Khandelwal et al., 2020)\nretrieves for every token, RETRO or IC-RALM\n(Borgeaud et al., 2022; Ram et al., 2023) retriev-\ners every 4∼32 tokens, and IRCoT (Trivedi et al.,']"," The title of the paper mentioned in the context information is ""Cache locality is not enough: High"
19,"What are the three sections into which the 14th century work ""The Divine Comedy"" is divided?","['Answer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\n1963. So the answer is August 25, 1963.\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\nAnswer (with step-by-step): The ﬁlm Twenty Plus Two was directed by Joseph M. Newman.\nThe Gal Who Took\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\nNow, combine the aforementioned two skills.\nFirst, write out the reasoning steps, then draw the conclusion,', 'Hint: This question is ambiguous in terms of the time frame or period being referred to.\nAnswer: In order to ﬁgure out its interpretations,\nPrompt C.8: exemplars of WikiAsp\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\n# Location\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\nat 39°56’12""N 32°51’55""E.\n# History\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸Serafettin after whom the mosque was named.\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\nGenerate a summary about Untold Legends:', 'born ﬁrst. So the answer is Martin Hodge.\nQuestion: When did the director of ﬁlm Laughter In Hell die?\nAnswer: The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\nanswer is August 25, 1963.\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\nAnswer: The ﬁlm Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\nQuestion: Which country the director of ﬁlm Citizen Mavzik is from?\nAnswer:', 'nationality?\nAnswer (with step-by-step): Coolie No. 1 (1995 ﬁlm) was directed by David Dhawan. The Sensational Trial was directed by\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\nnationality. So the answer is no.\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\nQuestion: Who was born ﬁrst out of Martin Hodge and Ivania Martinich?\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\nMartin Hodge was born ﬁrst. So the answer is Martin Hodge.\nQuestion: When did the director of ﬁlm Laughter In Hell die?\nAnswer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\n1963. So the answer is August 25, 1963.']"," The 14th century work ""The Divine Comedy"" is divided into three sections:"
20,What are the two components of RAG models described in the context?,"['on the four Pile datasets are presented in Table 4.\nConsistent with the findings on the Wikitext-103\ndataset, our model demonstrates superior perfor-\nmance compared to the baseline models across all\nfour datasets. It is important to note that we did\nnot finetune the client model specifically on the\nPile datasets, further highlighting the model’s gen-\neralization capabilities. Lastly, we have also ob-\nserved a significantly high perplexity when using\nthe GPT3 Davinci model. This is due to substan-\ntial variance in probability between the model’s\npredictions (generated by GPT3 zero-shot) and the\nground truth labels (generated by GPT3 with aug-\nmented memory).\n4.2.2\nInference Latency\nWe performed a latency evaluation for both the\nOPT-125M and OPT-350M models on the two hard-\nware setups, as described in Section 4.1.2. Figure\n5a illustrates that the OPT-125M model exhibits a\n49.3% faster inference time compared to the OPT-\n350M model. This finding emphasizes that the size', '27.1\n24.5\n23.3\n84.4\nVanilla OPT\n5.5\n9.7\n7.2\n22.2\n20.0\n19.1\n82.4\nRAG\n3.8\n14.8\n12.9\n28.4\n25.3\n25.1\n84.7\nHybridRAG w/o FT\n2.8\n20.1\n19.9\n32.9\n30.4\n29.7\n85.1\nHybridRAG\n2.8\n21.7\n21.1\n34.6\n31.8\n31.9\n85.9\nTable 6: Utility Performance of OPT-350M Model on Pile datasets\nPPL\nGLEU\nBLEU-4\nROUGE-1\nROUGE-L\nMETEOR\nBERTScore\nEnron\nEmails\nGPT3 zero-shot\n106.9\n12.3\n10.4\n26.1\n23.3\n21.6\n83.6\nVanilla OPT\n6.0\n10.5\n9.1\n21.5\n19.3\n18.0\n80.3\nRAG\n3.7\n12.7\n11.9\n25.2\n22.9\n21.6\n80.4\nHybridRAG w/o FT\n3.2\n20.3\n19.9\n31.0\n28.6\n27.4\n82.7\nHybridRAG\n3.7\n18.9\n18.9\n31.6\n28.3\n28.2\n83.8\nNIH\nExPorter\nGPT3 zero-shot\n12.2\n18.5\n16.2\n36.6\n31.7\n29.2\n86.7\nVanilla OPT\n5.4\n12.0\n10.8\n27.9\n25.3\n21.4\n84.3\nRAG\n3.8\n11.5\n10.5\n25.4\n23.3\n18.6\n83.8\nHybridRAG w/o FT\n2.9\n19.5\n19.8\n33.5\n31.4\n27.2\n85.6\nHybridRAG\n3.3\n23.0\n23.5\n36.2\n33.2\n30.8\n85.9\nHacker\nNews\nGPT3 zero-shot\n65.1\n15.3\n14.3\n30.2\n27.7\n20.4\n85.8\nVanilla OPT\n7.8\n11.8\n7.5\n29.7\n25.2\n22.2\n84.7\nRAG\n4.7\n15.8\n12.8\n34.1\n29.1\n27.2\n85.8\nHybridRAG w/o FT\n3.8\n19.5\n17.5\n37.3\n32.6\n30.8\n86.4\nHybridRAG', 'Epi-0\nEpi-1\nEpi-2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n(b) ADist Att. Score.\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) MoMA Doc Ratio.\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) ADist Doc Ratio.\nFigure 2: Grounding component breakdown for different distillation methods in each learning iteration. We display\nthe regularized doc and att. score ratio of documents from different augmentation sources.\nNQ\nHotpotQA\nFEVER\n0\n20\n40\n60\n80\n100\nTarget\nWiki\nMeSH\n(a) Doc Ratio. (Wiki)\nNFCorpus\nTREC-Covid\nBIOASQ\n0\n20\n40\n60\n80\n100\n(b) Doc Ratio. (Med)\nNQ\nHotpotQA\nFEVER\n0\n20\n40\n60\n80\n100\n(c) Att. Score Ratio. (Wiki)\nNFCorpus\nTREC-Covid\nBIOASQ\n0\n20\n40\n60\n80\n100\n(d) Att. Score Ratio. (Med)\nFigure 3: The inclusion of Plug-In memory during testing (grouped by the Wiki and Medical domains).\nto MeSH documents, especially on TREC-Covid task\nsince MeSH includes high quality updated information\nrelated to COVID-19. Wikipedia documents received\nmore attention on the Wiki-centric tasks like FEVER, as', 'the search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given\na test case, we ask LMs to combine skills 1 and 2 to\ngenerate search queries while performing the task.\nThe structure of the prompt is shown in Prompt 3.1,\nand further details can be found in Prompt C.1.\nPrompt 3.1: retrieval instructions\nSkill 1. An instruction to guide LMs to generate search\nqueries.\nSeveral search-related exemplars.\nSkill 2.\nAn instruction to guide LMs to perform a\nspeciﬁc downstream task (e.g., multihop QA).\nSeveral task-related exemplars.\nAn instruction to guide LMs to combine skills 1\nand 2 for the test case.\nThe input of the test case.\nAs shown in Figure 2, when the LM generates\n“[Search(query)]” (shown in gray italic), we stop\nthe generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next']", The two components of RAG models described in the context are the regularized document and attention score ratio
21,What is the title of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']"," The title of the paper on Retrieval Augmented Generation is ""Forward-Looking Active"
22,What is the benchmark dataset used for question answering research mentioned in the provided context?,"['munity QA6 with 2 billion question-answer pairs then\nﬁne-tuned on NQ and MS Marco dataset. In addition,\nthey use the hard negatives released by RocketQA (Qu\net al., 2021) when ﬁnetuning with MS Marco data and\nthe hard negatives release by (Lu et al., 2021) for Nat-\nural Questions. GTRbase leverages the same T5-base\nmodel as MoMA, while GTRlarge is based on T5-large,\nwhich is not directly comparable to our method as it\ntriples the parameters.\nDense Retrieval with Generated Queries\nGenQ\nﬁrst ﬁne-tunes a T5-base (Raffel et al., 2019) model on\nMS MARCO for 2 epochs and then generate 5 queries\n6Unfortunately, this corpus has not been released by the\nauthors.\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\npaper (Thakur et al., 2021b).\nSplit (→)\nTrain\nDev\nTest\nAvg. Word Lengths\nTask (↓)\nDomain (↓)\nDataset (↓)\nTitle\nRelevancy\n#Pairs\n#Query\n#Query\n#Corpus\nAvg. D / Q\nQuery\nDocument\nPassage-Retrieval\nMisc.\nMS MARCO\n\x17\nBinary\n532,761\n—-', 'Zichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', 'Table 8.\nA.2\nBaselines\nWe use the baselines from the current BEIR leader-\nboard (Thakur et al., 2021b) and recent papers. These\nbaselines can be divided into four groups: dense re-\ntrieval, dense retrieval with generated queries5, lexical\nretrieval and late interaction.\n4https://github.com/beir-cellar/beir\n5We separate them from dense retrieval since they usually\nrely on Seq2seq models to generate pseudo query-document\npairs, and they train a model for each dataset independently\ninstead of using a single model for all datasets.\nDense Retrieval\nFor dense retrieval, the baselines\nare the same dual-tower model as ours. We consider\nDPR (Karpukhin et al., 2020), ANCE (Xiong et al.,\n2020), T5-ANCE, coCondenser (Gao and Callan,\n2022) and one recently-proposed model GTR (Ni et al.,\n2021) with different size conﬁguration in this paper.\n• DPR uses a single BM25 retrieval example and in-\nbatch examples as hard negative examples to train\nthe model. Different from the original paper (Thakur', 'dataset for medical information retrieval. In Euro-\npean Conference on Information Retrieval, pages\n716–722. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nClaudio Carpineto and Giovanni Romano. 2012. A\nsurvey of automatic query expansion in information\nretrieval. Acm Computing Surveys (CSUR), 44(1):1–\n50.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1870–1879.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel Weld. 2020.\nSPECTER:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-informed transformers.\nIn Proceedings\nof the 58th Annual Meeting of the Association for']", The benchmark dataset used for question answering research mentioned in the provided context is MS MARCO.
23,What are the two models proposed in the paper on Retrieval Augmented Generation?,"['throughout the generation process, and are appli-\ncable to a variety of long-form generation tasks?\nWe consider a new paradigm, active retrieval aug-\nmented generation. Our hypothesis regarding when\nto retrieve is that LMs should retrieve information\nonly when they lack the required knowledge to\navoid unnecessary or inappropriate retrieval that\noccurs in passive retrieval-augmented LMs (Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\net al., 2023; Trivedi et al., 2022). Given the obser-\nvation that large LMs tend to be well-calibrated and\nlow probability/conﬁdence often indicates a lack\nof knowledge (Jiang et al., 2021; Kadavath et al.,\n2022), we adopt an active retrieval strategy that\nonly retrieves when LMs generate low-probability\ntokens. When deciding what to retrieve, we argue\nthat it is important to consider what LMs intend to\ngenerate in the future, as the goal of active retrieval\nis to beneﬁt future generations. Therefore, we pro-', 'resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', 'denoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-\nates the next sentence to gain insight into the future\ntopic, and if uncertain tokens are present, retrieves\nrelevant documents to regenerate the next sentence.\n3.1\nFLARE with Retrieval Instructions\nA straightforward way of expressing information\nneeds for retrieval is to generate “[Search(query)]”\nwhen additional information is needed (Schick\net al., 2023), e.g., “The colors on the ﬂag of\nGhana have the following meanings. Red is for\n[Search(Ghana ﬂag red meaning)] the blood of mar-\ntyrs, ...” When working with GPT-3.5 models that\noffer only API access, we elicit such behavior by\nfew-shot prompting (Brown et al., 2020).\nSpeciﬁcally, for a downstream task, we place\nthe search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']", The two models proposed in the paper on Retrieval Augmented Generation are FLAREinstruct
24,What is the approach used to train the retriever and generator components in the paper on Retrieval Augmented Generation?,"['boost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-']", The retriever and generator components in the paper on Retrieval Augmented Generation are trained
25,"What is the best performing ""closed-book"" open-domain QA model mentioned in the context?","['munity QA6 with 2 billion question-answer pairs then\nﬁne-tuned on NQ and MS Marco dataset. In addition,\nthey use the hard negatives released by RocketQA (Qu\net al., 2021) when ﬁnetuning with MS Marco data and\nthe hard negatives release by (Lu et al., 2021) for Nat-\nural Questions. GTRbase leverages the same T5-base\nmodel as MoMA, while GTRlarge is based on T5-large,\nwhich is not directly comparable to our method as it\ntriples the parameters.\nDense Retrieval with Generated Queries\nGenQ\nﬁrst ﬁne-tunes a T5-base (Raffel et al., 2019) model on\nMS MARCO for 2 epochs and then generate 5 queries\n6Unfortunately, this corpus has not been released by the\nauthors.\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\npaper (Thakur et al., 2021b).\nSplit (→)\nTrain\nDev\nTest\nAvg. Word Lengths\nTask (↓)\nDomain (↓)\nDataset (↓)\nTitle\nRelevancy\n#Pairs\n#Query\n#Query\n#Corpus\nAvg. D / Q\nQuery\nDocument\nPassage-Retrieval\nMisc.\nMS MARCO\n\x17\nBinary\n532,761\n—-', 'Among the 4 following options, which is\nthe correct answer?\n- A: {choice_A}\n- B: {choice_B}\n- C: {choice_C}\n- D: {choice_D}\nThe prompt template for PopQA is:\nQ: {question} A:\nB\nSelection of Source Task\nWe provide a detailed selection of the source tasks\nhere, using a variety of source and target tasks to an-\nalyze. MSMARCO QA, KILT-TriviaQA, and NQ\nbelong to Open Domain QA, while KILT-T-REx\nand zsRE belong to Slot Filling. MMLU belongs\nto Multi-task Language Understanding, which is\n3https://huggingface.co/OpenMatch/t5-ance\n4https://huggingface.co/facebook/contriever-msmarco\nTs\nTt\nMMLU\nNQ\nzsRE\nMSMARCO QA\n44.8\n46.7\n75.1\nKILT-TriviaQA\n43.6\n46.4\n74.9\nKILT-T-REx\n44.1\n45.9\n77.2\nTable 5: Relationship between the selection of source\ntask Ts and the performance of target task Tt. The\nmodel is Flan-T5Base w/ AARANCE. As NQ and zsRE\nare included in the Flan-T5 training data, we only report\ntheir F1 results here for reference.\ncloser to the Open Domain QA in terms of the task', 'and compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.\n6.1\nComparison with Baselines\nOverall results.\nThe overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks and datasets, multihop QA\nshows the most signiﬁcant improvement. This is\nlargely due to the task’s clear deﬁnition and speciﬁc\nobjective of producing the ﬁnal answer through a\n2-hop reasoning process, which makes it easier\nfor LMs to generate on-topic output. In contrast,\nASQA and WikiAsp are less clearly deﬁned and\nmore open-ended, which increases the difﬁculty of\n6To avoid leaking, we exclude several Wikipedia-related', 'model is Flan-T5Base w/ AARANCE. As NQ and zsRE\nare included in the Flan-T5 training data, we only report\ntheir F1 results here for reference.\ncloser to the Open Domain QA in terms of the task\nobjective. As shown in Table 5, when we align the\ncategory of the source task with the target task, the\nLM w/ AAR can generally achieve the best results.\nWe suppose that this is because LM may share sim-\nilar document preferences on the tasks from the\nsame dataset category, making AAR easier to gen-\neralize. Furthermore, taking MSMARCO QA as\nthe source task performs the best on MMLU. This\nvalidates the rationality to set Ts as MSMARCO\nQA in our main experimental settings.\nC\nAAR’s Improvements on PopQA\n250M\n780M\n3B\n175B\n# Parameters\n25\n30\n35\n40\n45\n50\n55\nPopQA Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\nFigure 8: AAR’s improvements on PopQA, using Flan-\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\nInstructGPT (175B) as target LMs.\nD\nFine-tuning Results']"," The best performing ""closed-book"" open-domain QA model mentioned in the context is FL"
26,What is the ratio of distinct to total tri-grams for the generation tasks in the Jeopardy Question Generation Task?,"['erates questions for low-conﬁdence spans without\nadditional annotation. Speciﬁcally, We ﬁrst extract\nall spans from ˆst with probabilities below β. For\neach extracted span z, we prompt gpt-3.5-turbo\nto generate a question qt,z that can be answered\nwith the span, using the following prompt:\nPrompt 3.2: zero-shot question generation\nUser input x.\nGenerated output so far y≤t.\nGiven the above passage, ask a question to which\nthe answer is the term/entity/phrase “z”.\nWe retrieve using each generated question and\ninterleave the returned documents into a single\nranking list to aid future generations. In summary,\nqueries qt are formulated based on ˆst as follows:\nqt =\n�\n∅\nif all tokens of ˆst have probs ≥ θ\nmask(ˆst) or qgen(ˆst)\notherwise\n3.3\nImplementation Details\nWe validate our method using one of the most ad-\nvanced GPT-3.5 LMs text-davinci-003 by itera-\ntively querying their API.2\n2https://api.openai.com/v1/completions in April\n2023.\nThe initial query.\nFLARE starts with the user', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and', 'and compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.\n6.1\nComparison with Baselines\nOverall results.\nThe overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks and datasets, multihop QA\nshows the most signiﬁcant improvement. This is\nlargely due to the task’s clear deﬁnition and speciﬁc\nobjective of producing the ﬁnal answer through a\n2-hop reasoning process, which makes it easier\nfor LMs to generate on-topic output. In contrast,\nASQA and WikiAsp are less clearly deﬁned and\nmore open-ended, which increases the difﬁculty of\n6To avoid leaking, we exclude several Wikipedia-related', 'lected based on the development set and listed in\nTable 8. FLARE refers to FLAREdirect if not specif-\nically stated. For previous-window approaches,\nwe follow Ram et al. (2023) to use a window size\nl = 16 in our experiments.\n5.1\nMultihop QA\nDataset\nThe goal of multihop QA is to answer\ncomplex questions through a process of informa-\ntion retrieval and reasoning (Yang et al., 2018; Ho\net al., 2020). For instance, to answer “Why did\nthe founder of Versus die?”, we must ﬁrst identify\nwho founded Versus and subsequently determine\nthe cause of their death. Multihop QA also uniﬁes\ninto long-form generation when solved with the\nstate-of-the-art CoT methods (Wei et al., 2022).\nWe use 2WikiMultihopQA (Ho et al., 2020)\nwhich contains 2-hop complex questions sourced\nfrom Wikipedia articles that require composition,\ncomparison, or inference.\nSettings\nWe follow Wang et al. (2022) to gen-\nerate both the chain-of-thought reasoning process\nand the ﬁnal answer. For the above case, the output']", The ratio of distinct to total trigrams for the generation tasks in the Jeopardy
27,What is the main finding of the paper on Retrieval Augmented Generation?,"['resulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and\npreviously generated output y<t = [y0, ..., yt−1]:\nqt = qry(x, y<t),\nwhere qry(·) is the query formulation function. At\nthe start of the generation (t = 1), the previous\ngeneration is empty (y<1 = ∅), and the user input\nis used as the initial query (q1 = x). Given the re-\ntrieved documents Dqt, LMs continually generate\nthe answer until the next retrieval is triggered or\nreaches the end:\nyt = LM([Dqt, x, y<t]),\nwhere yt represents the generated tokens at the\ncurrent step t, and the input to LMs is the concate-\nnation of the retrieved documents Dqt, the user\ninput x, and the previous generation y<t. At each\nstep, we discard previously retrieved documents\n∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3', '∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'throughout the generation process, and are appli-\ncable to a variety of long-form generation tasks?\nWe consider a new paradigm, active retrieval aug-\nmented generation. Our hypothesis regarding when\nto retrieve is that LMs should retrieve information\nonly when they lack the required knowledge to\navoid unnecessary or inappropriate retrieval that\noccurs in passive retrieval-augmented LMs (Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\net al., 2023; Trivedi et al., 2022). Given the obser-\nvation that large LMs tend to be well-calibrated and\nlow probability/conﬁdence often indicates a lack\nof knowledge (Jiang et al., 2021; Kadavath et al.,\n2022), we adopt an active retrieval strategy that\nonly retrieves when LMs generate low-probability\ntokens. When deciding what to retrieve, we argue\nthat it is important to consider what LMs intend to\ngenerate in the future, as the goal of active retrieval\nis to beneﬁt future generations. Therefore, we pro-']", The main finding of the paper on Retrieval Augmented Generation is that LMs can effectively
28,What is the main objective of the work presented in the paper on Retrieval Augmented Generation?,"['∪t′<tDqt′ and only use the retrieved documents\nfrom the current step to condition the next gener-\nation to prevent reaching the input length limit of\nLMs.\n3\nFLARE: Forward-Looking Active\nREtrieval Augmented Generation\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should re-\nﬂect the intents of future generations. Therefore,\nWe propose two forward-looking active retrieval\naugmented generation (FLARE) methods to im-\nplement the active retrieval augmented generation\nframework. Inspired by Toolformer (Schick et al.,\n2023), the ﬁrst method prompts the LM to generate\nretrieval queries when necessary while generating\nthe answer using retrieval-encouraging instructions,\ndenoted as FLAREinstruct. The second method di-\nrectly uses the LM’s generation as search queries,\ndenoted as FLAREdirect, which iteratively gener-', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search', 'throughout the generation process, and are appli-\ncable to a variety of long-form generation tasks?\nWe consider a new paradigm, active retrieval aug-\nmented generation. Our hypothesis regarding when\nto retrieve is that LMs should retrieve information\nonly when they lack the required knowledge to\navoid unnecessary or inappropriate retrieval that\noccurs in passive retrieval-augmented LMs (Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\net al., 2023; Trivedi et al., 2022). Given the obser-\nvation that large LMs tend to be well-calibrated and\nlow probability/conﬁdence often indicates a lack\nof knowledge (Jiang et al., 2021; Kadavath et al.,\n2022), we adopt an active retrieval strategy that\nonly retrieves when LMs generate low-probability\ntokens. When deciding what to retrieve, we argue\nthat it is important to consider what LMs intend to\ngenerate in the future, as the goal of active retrieval\nis to beneﬁt future generations. Therefore, we pro-', 'the user input to aid future generation for both\nbaselines and our method for fair comparisons:\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\nlowing the speciﬁed order.\n2.2\nSingle-time Retrieval-Augmented\nGeneration\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once y = LM([Dx, x]) (Chen\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\nShi et al., 2023).\n2.3\nActive Retrieval Augmented Generation\nTo aid long-form generation with retrieval, we pro-\npose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\nresulting in the interleaving of retrieval and genera-\ntion. Formally, at step t(t ≥ 1), the retrieval query\nqt is formulated based on both the user input x and']", The main objective of the work presented in the paper on Retrieval Augmented Generation is to
29,What are the limitations of large pre-trained language models in accessing and manipulating knowledge in knowledge-intensive tasks?,"['Michael Zeng, and Meng Jiang. 2023.\nGenerate\nrather than retrieve: Large language models are\nstrong context generators. In ICLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong\nWang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.\nPlug-and-play knowledge injection for pre-trained\nlanguage models. In Proceedings of ACL.\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\nand Lichao Sun. 2023. A comprehensive survey on', 'et al., 2019; Brown et al., 2020; Smith et al., 2022), but\nwith diminishing return: linear improvements on down-\nstream metrics often require exponentially more parame-\nters and computing cost (Kaplan et al., 2020; Hoffmann\net al., 2022). Hence, scaling pretrained language mod-\nels in this way is economically unsustainable (Strubell\net al., 2020; Bender et al., 2021; Zhang et al., 2022).\nRetrieval augmented language models provide a\npromising alternative. They allow language models\nto efﬁciently access vast resources from an external cor-\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\nas a kind of “memory” they can refer to when making\npredictions, alleviating the need to memorize as much\n∗Work partly done during Suyu’s internship at Microsoft.\ninformation in their own network parameters (Roberts\net al., 2020). This open-book approach helps language\nmodels to better generalize on token prediction tasks and\nmachine translation (Khandelwal et al., 2019; Borgeaud', 'Kilt: a benchmark for knowl-\nedge intensive language tasks.\narXiv preprint\narXiv:2009.02252.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An optimized\ntraining approach to dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5835–5847, On-\nline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The', 'Riedel. KILT: a benchmark for knowledge intensive\nlanguage tasks. In Proceedings of NAACL, pages\n2523–2544.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21:140:1–140:67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP,\npages 5418–5426.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of NAACL, pages 2655–\n2671.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.\nMultitask prompted training enables zero-shot task\ngeneralization. In ICLR.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-']", Large pre-trained language models have limitations in accessing and manipulating knowledge in knowledge-intensive
30,What is the main objective of RAG in the experiments conducted in the paper on Retrieval Augmented Generation?,"['Zichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', 'throughout the generation process, and are appli-\ncable to a variety of long-form generation tasks?\nWe consider a new paradigm, active retrieval aug-\nmented generation. Our hypothesis regarding when\nto retrieve is that LMs should retrieve information\nonly when they lack the required knowledge to\navoid unnecessary or inappropriate retrieval that\noccurs in passive retrieval-augmented LMs (Khan-\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\net al., 2023; Trivedi et al., 2022). Given the obser-\nvation that large LMs tend to be well-calibrated and\nlow probability/conﬁdence often indicates a lack\nof knowledge (Jiang et al., 2021; Kadavath et al.,\n2022), we adopt an active retrieval strategy that\nonly retrieves when LMs generate low-probability\ntokens. When deciding what to retrieve, we argue\nthat it is important to consider what LMs intend to\ngenerate in the future, as the goal of active retrieval\nis to beneﬁt future generations. Therefore, we pro-', 'boost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various', 'the generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the next\nsearch query is generated or reaches the end.\nSearch results:   !!\n[1]: …\n[2]: …\nJoe Biden attended\nSearch results:   !""!\n[1]: …\n[2]: …Search results:   !""""\n[1]: …\n[2]: …\n[Search(Joe Biden University)]\n[Search(Joe Biden degree)]\nthe University of Pennsylvania, where he earned\na law degree.\nGenerate a summary about Joe Biden.\nInput\n$\n&$\n&#\n%$\n&%\n%%\nGeneration\nRetriever\n$\n%$\n%%\nFigure 2: An illustration of forward-looking active re-\ntrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in gray italic) to retrieve relevant information\nto aid future generations.\nWe found that LMs can effectively combine the\ntwo skills and generate meaningful search queries\nwhile performing the task. However, there are\ntwo issues: (1) LMs tend to generate fewer search']", The main objective of RAG in the experiments conducted in the paper on Retrieval Augmented
31,What is the purpose of the MSMARCO NLG task v2.1?,"['model is Flan-T5Base w/ AARANCE. As NQ and zsRE\nare included in the Flan-T5 training data, we only report\ntheir F1 results here for reference.\ncloser to the Open Domain QA in terms of the task\nobjective. As shown in Table 5, when we align the\ncategory of the source task with the target task, the\nLM w/ AAR can generally achieve the best results.\nWe suppose that this is because LM may share sim-\nilar document preferences on the tasks from the\nsame dataset category, making AAR easier to gen-\neralize. Furthermore, taking MSMARCO QA as\nthe source task performs the best on MMLU. This\nvalidates the rationality to set Ts as MSMARCO\nQA in our main experimental settings.\nC\nAAR’s Improvements on PopQA\n250M\n780M\n3B\n175B\n# Parameters\n25\n30\n35\n40\n45\n50\n55\nPopQA Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\nFigure 8: AAR’s improvements on PopQA, using Flan-\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\nInstructGPT (175B) as target LMs.\nD\nFine-tuning Results', 'sponding metrics. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n2WikiMultihopQA\nASQA-hint\nEM\nF1\nPrec. Rec.\nEM D-F1 R-L DR\nPrevious 39.0 49.2 48.9 51.8\n42.5 34.1 36.9 35.5\nNext\n48.8 57.6 57.1 60.5\n45.9 35.7 37.5 36.6\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\nImportance of active retrieval.\nNext, we inves-\ntigate the relationship between performance and\nthe active retrieval threshold θ. To alter our method\nfrom not retrieving anything to retrieving every sen-\ntence, we adjusted the conﬁdence threshold θ used\nto determine when to trigger retrieval from 0 to\n1. We calculate the percentage of steps/sentences\nwhere retrieval is triggered for every threshold and\ndisplay the performance based on the percentage\nof retrieval. As shown in Figure 5, on 2WikiMul-\ntihopQA, the performance plateaus when the re-\ntrieval percentage exceeds 60%, indicating that re-\ntrieval when LMs are conﬁdent is not necessary.', 'and compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\nrun ablation experiments to study the efﬁcacy of\nvarious design choices of our method.\n6.1\nComparison with Baselines\nOverall results.\nThe overall performance of\nFLARE and baseline across all tasks/datasets are\nreported in Figure 4. FLARE outperforms all base-\nline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks and datasets, multihop QA\nshows the most signiﬁcant improvement. This is\nlargely due to the task’s clear deﬁnition and speciﬁc\nobjective of producing the ﬁnal answer through a\n2-hop reasoning process, which makes it easier\nfor LMs to generate on-topic output. In contrast,\nASQA and WikiAsp are less clearly deﬁned and\nmore open-ended, which increases the difﬁculty of\n6To avoid leaking, we exclude several Wikipedia-related', 'based summarization. Trans. Assoc. Comput. Lin-\nguistics, 9:211–225.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2020. Measuring massive multitask language\nunderstanding. CoRR, abs/2009.03300.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 6609–6625. International Committee on\nComputational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n874–880. Association for Computational Linguis-\ntics.']", The MSMARCO NLG task v2.1 is a natural language generation task that aims
