{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we'll leverage the ArxivLoader to load some papers, and then split them into more manageable chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware of your model's context window - and how many input tokens you allowed when creating your endpoint!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,# the character length of the chunk\n",
    "    chunk_overlap = 200,# the character length of the overlap between chunks\n",
    "    length_function =len # the length function - in this case, character length (aka the python len() fn.)\n",
    ")\n",
    "\n",
    "split_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage sentence-transformers models through Hugging Face to handle all our embedding needs locally!\n",
    "\n",
    "Keep in mind that if you're using a GPU instance, you'll be able to set device to cuda. Otherwise you should set it to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shubham.agnihotri/anaconda3/envs/llmops/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': \"cpu\"}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the same as we would with OpenAI's embeddings model - we can instantiate our FAISS vector store with our documents and our HuggingFaceEmbeddings model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    embedding=hf_embeddings,\n",
    "    documents=split_chunks,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is set up a RetrievalQA pipeline - and away we go!\n",
    "\n",
    "Remember your model's context window and the allowed number of input tokens you set up when creating the endpoint when setting a value for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id = 'mistralai/Mistral-7B-Instruct-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "k = 4\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm= llm,\n",
    "    retriever=faiss_vectorstore.as_retriever(search_kwargs={\"k\" : k})\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/megatron/RAQA_Mistral_movie_review_imdb/runs/osvtqpnm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Retrieval Augmented Generation?',\n",
       " 'result': ' Retrieval Augmented Generation is a method of generating text by combining a retrieval model with'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"What process is used to update the model's weights?\",\n",
       " 'result': \" The process used to update the model's weights is not specified in the provided context.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"What process is used to update the model's weights?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Our Open Source RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage RAGAS to evaluate our RAG pipeline using RAGAS.\n",
    "\n",
    "We'll use a preset list of questions - though you can reference the previous week's assignment if you want to build your own test questions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = ['What is the title of the paper on Retrieval Augmented Generation?', 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context?', 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?', 'What is the title of the paper on Retrieval Augmented Generation?', 'What is the task of the Retrieval Augmented Generation (RAG) model according to the context information?', 'What advantages does RAG have over the DPR QA system in terms of generating answers?', 'What are some potential downsides of using RAG, a language model based on Wikipedia, in various scenarios?', 'What is the file name of the paper on Retrieval Augmented Generation?', 'What is the accuracy of RAG models in generating correct answers even when the correct answer is not in any retrieved document?', 'Question: What is an example of how parametric and non-parametric memories work together in BART?', 'What is the title of the paper on Retrieval Augmented Generation?', 'What is the purpose of using multiple answer annotations in open-domain QA?', 'Question: Which novel by Ernest Hemingway is based on his wartime experiences?', 'What advantage do non-parametric memory models like RAG have over parametric-only models like T5 or BART?', 'What is the title of the paper on Retrieval Augmented Generation?', 'What is the title of the paper on Retrieval Augmented Generation?', 'What training setup details are mentioned in the paper on Retrieval Augmented Generation?', 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?', 'What is the title of the paper mentioned in the context information?', 'What are the three sections into which the 14th century work \"The Divine Comedy\" is divided?', 'What are the two components of RAG models described in the context?', 'What is the title of the paper on Retrieval Augmented Generation?', 'What is the benchmark dataset used for question answering research mentioned in the provided context?', 'What are the two models proposed in the paper on Retrieval Augmented Generation?', 'What is the approach used to train the retriever and generator components in the paper on Retrieval Augmented Generation?', 'What is the best performing \"closed-book\" open-domain QA model mentioned in the context?', 'What is the ratio of distinct to total tri-grams for the generation tasks in the Jeopardy Question Generation Task?', 'What is the main finding of the paper on Retrieval Augmented Generation?', 'What is the main objective of the work presented in the paper on Retrieval Augmented Generation?', 'What are the limitations of large pre-trained language models in accessing and manipulating knowledge in knowledge-intensive tasks?', 'What is the main objective of RAG in the experiments conducted in the paper on Retrieval Augmented Generation?', 'What is the purpose of the MSMARCO NLG task v2.1?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's collect all the answers to our questions!\n",
    "\n",
    "Be sure to return the source documents to have context to mark with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : k}),\n",
    "    return_source_documents = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context?', 'result': ' FLARE: Forward-Looking Active REtrieval Augmented Generation', 'source_documents': [Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?', 'result': ' The title of the paper on Retrieval Augmented Generation mentioned in the context information is \"', 'source_documents': [Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='retrieved documents from C using the retriever it-\\nself (Xiong et al., 2020). The loss function l() can\\nbe any standard ranking loss such as cross entropy. A\\nZeroDR model is trained on qs and documents ds ∈ Cs\\nfrom a source task, often web search, and tested on tar-\\nget tasks qt and Ct; supervision signals are only present\\nfrom the source.\\nMixture-of-Memory Augmentation. The key idea\\nof (document-based) retrieval augmented language mod-\\nels is to enrich the representation g(q) with additional\\ncontextual input for the model, i.e., augmentation doc-\\numents da retrieved from an external memory M. In-\\nstead of using a single document corpus, MoMA uses\\nmultiple corpora to provide richer and more diverse ex-\\nternal resources for augmentation. For example, M\\ncan be composed by the source corpus Cs, a general\\nencyclopedia, a domain speciﬁc knowledge graph, etc.\\nThen we can retrieve the augmentation documents Da :\\nDa = ANNM\\nf a(x,◦); M = {C1, ..., CM}.\\n(4)', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the task of the Retrieval Augmented Generation (RAG) model according to the context information?', 'result': ' The task of the Retrieval Augmented Generation (RAG) model is to generate meaningful', 'source_documents': [Document(page_content='boost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}), Document(page_content='may beneﬁt the response generation, if xr (or yr)\\nis similar (or relevant) to the input x. It is worth\\nnoting that xr = ∅ when unsupervised retrieval\\nsources are used. In general, the retrieval mem-\\nory can be retrieved from three kinds of sources:\\nthe training corpus, external datasets in the same\\nformat with the training corpus, and large-scale\\nunsupervised corpus (§2.2). Metrics that evaluate\\nthe relevance between text are varied as well, in\\n§2.3 we divided them into three categories: sparse-\\nvector retrieval, dense-vector retrieval, and training-\\nbased retrieval. Finally, how to integrate the re-\\ntrieval memory to the generation model is also sig-\\nniﬁcant, we also introduce some popular integra-\\ntion approaches in §2.4.\\n2.2\\nRetrieval Sources\\nTraining Corpus\\nMost previous studies search\\nthe external memory from its training corpus (Song\\net al., 2016; Gu et al., 2018; Weston et al., 2018).\\nIn the inference time, retrieved examples with high', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='throughout the generation process, and are appli-\\ncable to a variety of long-form generation tasks?\\nWe consider a new paradigm, active retrieval aug-\\nmented generation. Our hypothesis regarding when\\nto retrieve is that LMs should retrieve information\\nonly when they lack the required knowledge to\\navoid unnecessary or inappropriate retrieval that\\noccurs in passive retrieval-augmented LMs (Khan-\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\net al., 2023; Trivedi et al., 2022). Given the obser-\\nvation that large LMs tend to be well-calibrated and\\nlow probability/conﬁdence often indicates a lack\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022), we adopt an active retrieval strategy that\\nonly retrieves when LMs generate low-probability\\ntokens. When deciding what to retrieve, we argue\\nthat it is important to consider what LMs intend to\\ngenerate in the future, as the goal of active retrieval\\nis to beneﬁt future generations. Therefore, we pro-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What advantages does RAG have over the DPR QA system in terms of generating answers?', 'result': ' RAG has several advantages over the DPR QA system in terms of generating answers. Firstly', 'source_documents': [Document(page_content='and compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.\\n6.1\\nComparison with Baselines\\nOverall results.\\nThe overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks and datasets, multihop QA\\nshows the most signiﬁcant improvement. This is\\nlargely due to the task’s clear deﬁnition and speciﬁc\\nobjective of producing the ﬁnal answer through a\\n2-hop reasoning process, which makes it easier\\nfor LMs to generate on-topic output. In contrast,\\nASQA and WikiAsp are less clearly deﬁned and\\nmore open-ended, which increases the difﬁculty of\\n6To avoid leaking, we exclude several Wikipedia-related', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='(Fan et al., 2019), and found that FLARE did not\\nprovide signiﬁcant gains. Wizard of Wikipedia is\\na knowledge-intensive dialogue generation dataset\\nwhere the output is relatively short (∼20 tokens\\non average) so retrieving multiple disparate pieces\\nof information might not be necessary. ELI5 (Fan\\net al., 2019) is a long-form QA dataset requiring\\nin-depth answers to open-ended questions. Due to\\nissues mentioned in Krishna et al. (2021) such as\\ndifﬁculties of grounding generation in retrieval and\\nevaluation, both single-time retrieval and FLARE\\ndid not provide signiﬁcant gains over not using\\nretrieval. From an engineering perspective, inter-\\nleaving generation with retrieval with a naive im-\\nplementation increases both overheads and the cost\\nof generation. The LM needs to be activated mul-\\ntiple times (once for each retrieval) and a caching-\\nfree implementation will also require recomputing\\nthe previous activation each time after a retrieval.', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='lected based on the development set and listed in\\nTable 8. FLARE refers to FLAREdirect if not specif-\\nically stated. For previous-window approaches,\\nwe follow Ram et al. (2023) to use a window size\\nl = 16 in our experiments.\\n5.1\\nMultihop QA\\nDataset\\nThe goal of multihop QA is to answer\\ncomplex questions through a process of informa-\\ntion retrieval and reasoning (Yang et al., 2018; Ho\\net al., 2020). For instance, to answer “Why did\\nthe founder of Versus die?”, we must ﬁrst identify\\nwho founded Versus and subsequently determine\\nthe cause of their death. Multihop QA also uniﬁes\\ninto long-form generation when solved with the\\nstate-of-the-art CoT methods (Wei et al., 2022).\\nWe use 2WikiMultihopQA (Ho et al., 2020)\\nwhich contains 2-hop complex questions sourced\\nfrom Wikipedia articles that require composition,\\ncomparison, or inference.\\nSettings\\nWe follow Wang et al. (2022) to gen-\\nerate both the chain-of-thought reasoning process\\nand the ﬁnal answer. For the above case, the output', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='LM (Ans-Deletion)\\n(b) Answer-deletion test.\\nFigure 6: Analysis of LM-preferred documents. (a)\\nshows the overlaps of positive document sets, where\\nused LMs are Flan-T5 series. (b) presents the answer-\\ndeletion experiments on the MSMARCO QA dataset.\\nThe retriever is initialized from ANCE.\\n(Dh+) and LMs (Top-KSa\\ni ,Da) are quite low (near\\n13%), demonstrating their distinct tendencies in\\nselecting valuable documents. On the contrary, the\\noverlaps between different LMs are relatively high\\n(over 55%). This evidence provides a strong ratio-\\nnale for the generalization ability of AAR since\\nLMs with different sizes tend to annotate simi-\\nlar positive documents. Furthermore, LMs whose\\nsizes are closer generally possess higher overlaps.\\nThis implies a better generalization ability of the\\nAAR to the LMs whose capacity is near the source\\nLM. The findings further validate the results illus-\\ntrated in Figure 4b.\\nTo give an in-depth analysis of how human-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"})]}\n",
      "{'query': 'What are some potential downsides of using RAG, a language model based on Wikipedia, in various scenarios?', 'result': '\\n\\nThere are several potential downsides of using RAG, a language model based on Wikipedia', 'source_documents': [Document(page_content='Gretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot learn-\\ners. In Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Informa-\\ntion Processing Systems 2020, NeurIPS 2020, De-\\ncember 6-12, 2020, virtual.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\\nAssociation for Computational Linguistics.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='On MMLU, using MS MARCO as the retrieval\\ncorpus improves the LM more compared to KILT-\\nWikipedia. We hypothesize that the retriever has\\nbeen trained with MS MARCO corpus and thus\\nholds better retrieval performance on it.\\nSettings\\nMethods\\nMMLU\\nPopQA\\nAll\\nAll\\nFew-shot\\nOPT (Zhang et al., 2022)\\n26.0\\n12.3\\nGPT-neo (Black et al., 2021)\\n28.7\\n11.3\\nZero-shot\\nOPT\\n22.7\\n12.0\\nGPT-neo\\n25.3\\n9.9\\nOPT GenRead\\n22.3\\n12.2\\nGPT-neo GenRead\\n24.4\\n11.9\\nOPT w/ AARContriever (Ours)\\n23.2\\n29.1\\nGPT-neo w/ AARContriever (Ours)\\n25.2\\n27.8\\nOPT w/ AARANCE (Ours)\\n23.7\\n32.9\\nGPT-neo w/ AARANCE (Ours)\\n26.6\\n30.1\\nTable 4: Results of OPT and GPT-neo. We use their\\n1.3B version. The score marked as bold means the best\\nperformance in the zero-shot setting.\\nOn PopQA, model performance will drop by\\nlarge margins if we use MS MARCO as the re-\\ntrieval corpus instead of KILT-Wikipedia. The pri-\\nmary reason is that the PopQA dataset is sampled\\nfrom Wikidata and designed for long-tail questions.', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='leverage the augmentation information from multiple\\ncorpora during training and testing. We hope our ﬁnd-\\nings and illustrations can inspire more future research in\\nbetter augmenting language models, to provide other al-\\nternatives to achieve generalization ability beyond solely\\nrelying on model scale.\\nLimitations\\nAlthough MoMA (T5-ANCE) and MoMA (COCO)\\nachieve strong zero-shot performances, we mainly ver-\\nify their efﬁcacy from the empirical performances\\non BEIR tasks, where the target corpora, Wiki and\\nMARCO serve as readily available retrieval sources.\\nIn a real-world scenario, the grounding corpora usually\\nneed to be customized according to query domains and\\nuser needs. Thus, how to choose effective grounding\\ncorpora and efﬁciently evaluate their relative contribu-\\ntion remain an open problem. These analyses will go\\nbeyond our empirical settings and reveal a wider appli-\\ncation scenario of MoMA.\\nEthics Statement\\nAll data in this study are publicly available and used', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='narayana Pillai, Marie Pellat, Aitor Lewkowycz,\\nErica Moreira, Rewon Child, Oleksandr Polozov,\\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\\nPalm: Scaling language modeling with pathways.\\nCoRR, abs/2204.02311.\\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\\nAlessandro Moschitti. 2021.\\nWikisum:\\nCoher-\\nent summarization dataset for efﬁcient human-\\nevaluation. In Proceedings of the 59th Annual Meet-\\ning of the Association for Computational Linguis-\\ntics and the 11th International Joint Conference on\\nNatural Language Processing, ACL/IJCNLP 2021,\\n(Volume 2: Short Papers), Virtual Event, August 1-\\n6, 2021, pages 212–219. Association for Computa-\\ntional Linguistics.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\nof wikipedia: Knowledge-powered conversational', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the file name of the paper on Retrieval Augmented Generation?', 'result': ' The file name of the paper on Retrieval Augmented Generation is \"FLARE: For', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='Zichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the accuracy of RAG models in generating correct answers even when the correct answer is not in any retrieved document?', 'result': ' The provided context does not mention the accuracy of RAG models in generating correct answers even when the correct', 'source_documents': [Document(page_content='LM (Ans-Deletion)\\n(b) Answer-deletion test.\\nFigure 6: Analysis of LM-preferred documents. (a)\\nshows the overlaps of positive document sets, where\\nused LMs are Flan-T5 series. (b) presents the answer-\\ndeletion experiments on the MSMARCO QA dataset.\\nThe retriever is initialized from ANCE.\\n(Dh+) and LMs (Top-KSa\\ni ,Da) are quite low (near\\n13%), demonstrating their distinct tendencies in\\nselecting valuable documents. On the contrary, the\\noverlaps between different LMs are relatively high\\n(over 55%). This evidence provides a strong ratio-\\nnale for the generalization ability of AAR since\\nLMs with different sizes tend to annotate simi-\\nlar positive documents. Furthermore, LMs whose\\nsizes are closer generally possess higher overlaps.\\nThis implies a better generalization ability of the\\nAAR to the LMs whose capacity is near the source\\nLM. The findings further validate the results illus-\\ntrated in Figure 4b.\\nTo give an in-depth analysis of how human-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='AAR to the LMs whose capacity is near the source\\nLM. The findings further validate the results illus-\\ntrated in Figure 4b.\\nTo give an in-depth analysis of how human-\\npreferred and LM-preferred documents differ, we\\nshow two representative cases sampled from the\\nMSMARCO QA in Table 2. We observe that the\\nhuman-preferred document can always present the\\ngold answer at the beginning of the text, while the\\nLM-preferred document may not contain the ex-\\nact answer. However, an LM-preferred document\\n250M\\n780M\\n3B\\n175B\\n# Parameters\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\nMMLU Accuracy\\nTART\\nAARContriever (MSMARCO QA)\\nAARContriever (KILT)\\nAARANCE (MSMARCO QA)\\nAARANCE (KILT)\\nFigure 7:\\nComparison between single-task (MS-\\nMARCO QA) and multi-task (KILT) trained AAR.\\nTART (Asai et al., 2022) is a multi-task instruction-\\nfinetuned retriever that has not been finetuned with LM-\\npreferred signals.\\ncan (1) deliver a new perspective to answer the\\ngiven question, e.g., “the cruise line’s responsibil-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='that our model consistently outperforms all the other baselines, demonstrating the superior performance.\\nHowever, it is worth noting that the perplexity of the OPT-125M model does not consistently surpass that\\nof the HybridRAG w/o FT model.\\nF\\nMore examples from human evaluation\\nTable 8 shows another working example for HybridRAG models and Table 9 shows an example of a\\nfailing case for both OPT-125M and OPT-350M HybridRAG models. The OPT-125M model generates\\na repetitive and factually incorrect text, while the OPT-350M model generates a text that is factually\\nincorrect. GPT3 Davinci, however, can still use the same retrieved memories to provide a factual and useful\\ncompletion for this prompt. The memories are bullet points generated from several document chunks;\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nEnron\\nEmails\\nGPT3 zero-shot\\n106.9\\n12.3\\n10.4\\n26.1\\n23.3\\n21.6\\n83.6\\nVanilla OPT\\n5.4\\n8.7\\n6.5\\n21.1\\n18.6\\n17.1\\n80.2\\nRAG\\n3.4\\n13.6\\n12.2\\n26.8\\n24.0\\n22.9\\n81.2\\nHybridRAG w/o FT\\n2.9\\n19.9\\n19.4', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='the open-domain setting by removing the associ-\\nated references and instead gathering information\\nfrom the open web. For the above case, the output\\nwe aim to generate is “# Academics. In 2008, 91%\\nof the school’s seniors received their high school\\ndiploma... # History. The class of 2008 was the\\n100th class in the school’s history.” where # is used\\nto indicate aspects. We manually annotate 4 exem-\\nplars (Prompt C.8), and use the Bing search engine\\nto retrieve 5 documents from the open web.6\\nEvaluation\\nWe compare system outputs with the\\ngold summary using ROUGE, named entity-based\\nF1, and UniEval (Zhong et al., 2022) which mea-\\nsures factual consistency based on prediction proba-\\nbility of a ﬁne-tuned T5 model (Raffel et al., 2020).\\n6\\nExperimental Results\\nWe ﬁrst report overall results across 4 tasks/datasets\\nand compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'Question: What is an example of how parametric and non-parametric memories work together in BART?', 'result': ' In the context of BART, parametric memory refers to the knowledge obtained from pre-training,', 'source_documents': [Document(page_content='that our model consistently outperforms all the other baselines, demonstrating the superior performance.\\nHowever, it is worth noting that the perplexity of the OPT-125M model does not consistently surpass that\\nof the HybridRAG w/o FT model.\\nF\\nMore examples from human evaluation\\nTable 8 shows another working example for HybridRAG models and Table 9 shows an example of a\\nfailing case for both OPT-125M and OPT-350M HybridRAG models. The OPT-125M model generates\\na repetitive and factually incorrect text, while the OPT-350M model generates a text that is factually\\nincorrect. GPT3 Davinci, however, can still use the same retrieved memories to provide a factual and useful\\ncompletion for this prompt. The memories are bullet points generated from several document chunks;\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nEnron\\nEmails\\nGPT3 zero-shot\\n106.9\\n12.3\\n10.4\\n26.1\\n23.3\\n21.6\\n83.6\\nVanilla OPT\\n5.4\\n8.7\\n6.5\\n21.1\\n18.6\\n17.1\\n80.2\\nRAG\\n3.4\\n13.6\\n12.2\\n26.8\\n24.0\\n22.9\\n81.2\\nHybridRAG w/o FT\\n2.9\\n19.9\\n19.4', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='Sherlock moved to Belsize Park in\\nVanilla OPT-350M\\n) was an English cricketer. He played first-class cricket for Surrey in the 1960s and 1970s.\\nReferences\\nExternal links\\nHybridRAG OPT-350M\\n) was a British comedian and actor. He was born in London, the son of a British diplomat.\\nHe was educated at Emmanuel College, Cambridge, where he studied medicine. He also\\njoined the Cambridge Footlights, where\\nTable 5: Completions of the prompt by different models, showcasing a working case for HybridRAG OPT-125M;\\nthis is also a working case for HybridRAG OPT-350M if we only consider the first sentence.\\nindicate that the HybridRAG OPT-125M has a bet-\\nter balance between the retrieval and generation\\nmechanisms.\\nHowever, we also observe cases where the Hy-\\nbridRAG model fails (refer to Appendix F). Im-\\nproving the memory generator by reducing dupli-\\ncate information, and enhancing the reasoning abil-\\nities of the client model or encouraging it to stick to', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='boost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}), Document(page_content='LMs, including Flan-T5 (Chung et al., 2022), In-\\nstructGPT (Ouyang et al., 2022), GAL (Taylor\\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\\nare prompted by a natural language instruction that\\ndescribes the desired task and question. Adaptive\\nretrieval (Mallen et al., 2022) selectively utilizes\\nnon-parametric memory (retrieval augmentation)\\nand parametric memory (the knowledge obtained\\nfrom pre-training) based on questions’ popularity.\\nIn our main experiment, we select the optimal com-\\nbination in their paper, which consists of Contriever\\nas the non-parametric memory and GenRead (Yu\\net al., 2023) as the parametric memory.\\nFew-shot Setting. We also include the results of\\nprevious few-shot models for reference. Flan-T5,\\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\\nand OPT-IML-Max adopt few-shot demonstrations,\\nwhich provide the LMs with a limited number of\\ntask examples. This enables the models to gener-\\nalize from these examples and generate accurate', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the purpose of using multiple answer annotations in open-domain QA?', 'result': ' Multiple answer annotations are used in open-domain QA to provide a comprehensive answer that covers', 'source_documents': [Document(page_content='lected based on the development set and listed in\\nTable 8. FLARE refers to FLAREdirect if not specif-\\nically stated. For previous-window approaches,\\nwe follow Ram et al. (2023) to use a window size\\nl = 16 in our experiments.\\n5.1\\nMultihop QA\\nDataset\\nThe goal of multihop QA is to answer\\ncomplex questions through a process of informa-\\ntion retrieval and reasoning (Yang et al., 2018; Ho\\net al., 2020). For instance, to answer “Why did\\nthe founder of Versus die?”, we must ﬁrst identify\\nwho founded Versus and subsequently determine\\nthe cause of their death. Multihop QA also uniﬁes\\ninto long-form generation when solved with the\\nstate-of-the-art CoT methods (Wei et al., 2022).\\nWe use 2WikiMultihopQA (Ho et al., 2020)\\nwhich contains 2-hop complex questions sourced\\nfrom Wikipedia articles that require composition,\\ncomparison, or inference.\\nSettings\\nWe follow Wang et al. (2022) to gen-\\nerate both the chain-of-thought reasoning process\\nand the ﬁnal answer. For the above case, the output', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='and compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.\\n6.1\\nComparison with Baselines\\nOverall results.\\nThe overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks and datasets, multihop QA\\nshows the most signiﬁcant improvement. This is\\nlargely due to the task’s clear deﬁnition and speciﬁc\\nobjective of producing the ﬁnal answer through a\\n2-hop reasoning process, which makes it easier\\nfor LMs to generate on-topic output. In contrast,\\nASQA and WikiAsp are less clearly deﬁned and\\nmore open-ended, which increases the difﬁculty of\\n6To avoid leaking, we exclude several Wikipedia-related', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='ing. In North American Association for Computa-\\ntional Linguistics.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: a benchmark for question answer-\\ning research.\\nTrans. Assoc. Comput. Linguistics,\\n7:452–466.\\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\\njape, Christopher D. Manning, and Kyoung-Gu Woo.\\n2021. You only need one model for open-domain\\nquestion answering. CoRR, abs/2112.07381.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020.\\nRetrieval-augmented generation for\\nknowledge-intensive NLP tasks.\\nIn Advances in', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='The following question “Where do the Philadelphia\\nEagles play their home games?” could be asking\\nabout the city, sports complex, or stadium of their\\nhome games. We use ASQA (Stelmakh et al., 2022)\\nas our testbed where inputs are ambiguous ques-\\ntions with multiple interpretations, and outputs are\\ncomprehensive answers covering all.\\nSetting\\nTo answer ambiguous questions, systems\\nmust ﬁrst identify possible interpretations and then\\nprovide answers for each of them, which for the\\nabove case is “We need to consider the different\\npossible locations or venues that could be con-\\nsidered the home ﬁeld of the Philadelphia Eagles.\\nThese include the city, the sports complex, or the\\nstadium. Therefore, this question has 3 interpreta-\\ntions and the answers are: (1) The city is Philadel-\\nphia. (2) The sports complex is the South Philadel-\\nphia Sports Complex. (3) The stadium is the Lin-\\ncoln Financial Field stadium.” We found that in\\nmany cases, it is challenging even for humans to', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'Question: Which novel by Ernest Hemingway is based on his wartime experiences?', 'result': ' The novel “For Whom the Bell Tolls” is based on Ernest Hemingway’', 'source_documents': [Document(page_content='January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.\\nPrompt C.5: exemplars of ASQA (cont.)\\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\\nQuestion: What was roy orbison’s ﬁrst number one hit?\\nAnswer: This question is ambiguous in terms of which speciﬁc chart or region is being referred to. In order to ﬁgure out its\\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\\nwas Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s ﬁrst number one hit in\\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the ﬁrst number one hit for Roy Orbison\\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the ﬁrst number one hit for Roy Orbison in', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='Answer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.\\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer (with step-by-step): The ﬁlm Twenty Plus Two was directed by Joseph M. Newman.\\nThe Gal Who Took\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nNow, combine the aforementioned two skills.\\nFirst, write out the reasoning steps, then draw the conclusion,', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='left. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\\ndestroyed fort was replaced with the present day Castillo de San Marcos.\\n# Background\\nWar had already been unofﬁcially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\\nEngland, he struck ﬁrst at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before ﬁnally visiting and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\\nquestion has 2 interpretations: (1) What was Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What\\nwas Roy Orbison’s ﬁrst number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\\nwas the ﬁrst number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\\nthe ﬁrst number one hit for Roy Orbison in the UK and Ireland.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nHint: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\\nname.\\nAnswer: In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What advantage do non-parametric memory models like RAG have over parametric-only models like T5 or BART?', 'result': ' Non-parametric memory models like RAG have the advantage of being able to store and retrieve information', 'source_documents': [Document(page_content='lions of parameters are able to capture a signif-\\nicant amount of human knowledge, leading to\\nconsistent improvements on various downstream\\ntasks (Brown et al., 2020; Kaplan et al., 2020;\\nRoberts et al., 2020). However, the undeniable\\ndrawback of large LMs lies in their high compu-\\ntational cost, which negatively impacts their effi-\\nciency (Strubell et al., 2019; Bender et al., 2021).\\nFurthermore, the knowledge memorized from pre-\\ntraining and the implicit reasoning process of LMs\\ncan be inaccurate and intractable sometimes, hin-\\ndering their applications on knowledge-intensive\\ntasks (Guu et al., 2020; Lewis et al., 2020; Mallen\\net al., 2022; Wei et al., 2022).\\nFlan-T5Base\\n(250M)\\nFlan-T5Large\\n(780M)\\nFlan-T5XL\\n(3B)\\nInstructGPT\\n(175B)\\n# Parameters\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\nMMLU Accuracy\\nStandalone LM\\nLM w/ Few-Shot Prompting\\nLM w/ Adaptive Retrieval\\nLM w/ AAR (Ours)\\nFigure 1: Performance of LM w/ AAR (Ours).\\nInstead of leveraging the knowledge and rea-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='LMs, including Flan-T5 (Chung et al., 2022), In-\\nstructGPT (Ouyang et al., 2022), GAL (Taylor\\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\\nare prompted by a natural language instruction that\\ndescribes the desired task and question. Adaptive\\nretrieval (Mallen et al., 2022) selectively utilizes\\nnon-parametric memory (retrieval augmentation)\\nand parametric memory (the knowledge obtained\\nfrom pre-training) based on questions’ popularity.\\nIn our main experiment, we select the optimal com-\\nbination in their paper, which consists of Contriever\\nas the non-parametric memory and GenRead (Yu\\net al., 2023) as the parametric memory.\\nFew-shot Setting. We also include the results of\\nprevious few-shot models for reference. Flan-T5,\\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\\nand OPT-IML-Max adopt few-shot demonstrations,\\nwhich provide the LMs with a limited number of\\ntask examples. This enables the models to gener-\\nalize from these examples and generate accurate', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='on the four Pile datasets are presented in Table 4.\\nConsistent with the findings on the Wikitext-103\\ndataset, our model demonstrates superior perfor-\\nmance compared to the baseline models across all\\nfour datasets. It is important to note that we did\\nnot finetune the client model specifically on the\\nPile datasets, further highlighting the model’s gen-\\neralization capabilities. Lastly, we have also ob-\\nserved a significantly high perplexity when using\\nthe GPT3 Davinci model. This is due to substan-\\ntial variance in probability between the model’s\\npredictions (generated by GPT3 zero-shot) and the\\nground truth labels (generated by GPT3 with aug-\\nmented memory).\\n4.2.2\\nInference Latency\\nWe performed a latency evaluation for both the\\nOPT-125M and OPT-350M models on the two hard-\\nware setups, as described in Section 4.1.2. Figure\\n5a illustrates that the OPT-125M model exhibits a\\n49.3% faster inference time compared to the OPT-\\n350M model. This finding emphasizes that the size', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='model for generation. (iii) HybridRAG without fine-\\ntuning (HybridRAG w/o FT): To assess the efficacy\\nof our instruction-tuned client model, we examine\\na HybridRAG model without applying finetuning\\nto the cilent model for text prediction. (iv) GPT3\\nzero-shot: We use the GPT3 Davinci model in a\\nzero-shot manner for text completion. However,\\nit’s important to note that the GPT3 model cannot\\nbe deployed on client devices for real-time compo-\\nsition assistance.\\nWhen evaluating the baseline models, we ensure\\na fair comparison by regenerating reference labels\\nusing the GPT3 model, based on the memory used\\n3https://en.wikipedia.org/wiki/Perplexity\\nby each baseline. Specifically, for the Vanilla OPT\\nbaseline, reference labels are generated with GPT3\\nwithout additional memory. For RAG, reference\\nlabels are generated by GPT3 with full text. In the\\ncase of GPT3-zeroshot baseline, since there is no\\nideal reference label for comparison, we used the\\nsame label as our HybridRAG approach.\\n4.2', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What training setup details are mentioned in the paper on Retrieval Augmented Generation?', 'result': ' The paper on Retrieval Augmented Generation mentions the following training setup details:\\n\\n', 'source_documents': [Document(page_content='Zichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='margins with fewer training FLOPs.\\n5.2\\nAblation Study\\nIn this experiment, we conduct the ablation study of\\naugmentation-adapted training and analyze model\\nbehaviors during the training process.\\nFigure 4a illustrates that augmentation-adapted\\ntraining can bring additional improvements com-\\npared to the pre-trained retrievers.\\nIn general,\\nANCE benefits more from augmentation-adapted\\ntraining than Contriever. This may be due to the\\nfact that Contriever has been already intensively\\npre-trained on massive data augmentations as well\\nas MS MARCO whereas ANCE is trained only on\\nMS MARCO. We provide exact numbers in Table 7\\nand PopQA results in Figure 8, which yield similar\\nobservations as MMLU.\\nIn Figure 4b, we compare retrievers trained with\\ndifferent positive documents, including human-\\npreferred documents annotated by search users (the\\nblue bar), LM-preferred documents obtained by\\nthe source LM (the orange bar), and their combi-\\nnations (the green bar and the red bar). Since the', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='a memory-augmented client model (client), a re-\\ntriever model (cloud), a memory generator (cloud).\\nFigure 1 illustrates the model architecture. The\\naugmentation coordinator monitors the writing con-\\ntext and determines when to request an augmented\\nmemory from the cloud. The retriever model on\\nthe cloud server then searches the retrieval corpus\\nto find relevant data. Subsequently, the memory\\ngenerator employs the GPT3 model to construct\\nan augmented memory that includes all essential\\ninformation from the retrieved data, optimizing\\nits usefulness. Finally, the augmented memory is\\ntransmitted to the client and seamlessly integrated\\ninto the client model, thereby enhancing its overall\\nperformance.\\n3.2\\nAugmentation Coordinator\\nThe augmentation coordinator component is re-\\nsponsible for managing the augmented memory\\nM by monitoring changes to the writing context.\\nThe entire process of the augmentation coordinator\\nis depicted in Figure 2. To determine whether a', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation mentioned in the context information?', 'result': ' The title of the paper on Retrieval Augmented Generation mentioned in the context information is \"', 'source_documents': [Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='retrieved documents from C using the retriever it-\\nself (Xiong et al., 2020). The loss function l() can\\nbe any standard ranking loss such as cross entropy. A\\nZeroDR model is trained on qs and documents ds ∈ Cs\\nfrom a source task, often web search, and tested on tar-\\nget tasks qt and Ct; supervision signals are only present\\nfrom the source.\\nMixture-of-Memory Augmentation. The key idea\\nof (document-based) retrieval augmented language mod-\\nels is to enrich the representation g(q) with additional\\ncontextual input for the model, i.e., augmentation doc-\\numents da retrieved from an external memory M. In-\\nstead of using a single document corpus, MoMA uses\\nmultiple corpora to provide richer and more diverse ex-\\nternal resources for augmentation. For example, M\\ncan be composed by the source corpus Cs, a general\\nencyclopedia, a domain speciﬁc knowledge graph, etc.\\nThen we can retrieve the augmentation documents Da :\\nDa = ANNM\\nf a(x,◦); M = {C1, ..., CM}.\\n(4)', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'})]}\n",
      "{'query': 'What is the title of the paper mentioned in the context information?', 'result': ' The title of the paper mentioned in the context information is \"Cache locality is not enough: High', 'source_documents': [Document(page_content='FEVER\\n(Thorne\\net\\nal.,\\n2018),\\nand\\nClimate-\\nFEVER (Diggelmann et al., 2020)\\nWe list the statistics of the BEIR benchmark in Table 7.\\nAugmenting Corpora\\nCorpus size We ﬁrst introduce\\nmore details on how we preprocessed the Medical Sub-\\nject Headings (MeSH) Database. We select text in-\\nformation from the Qualiﬁer Record Set and Descrip-\\ntor Record Set. Each set contains multiple <Concept>\\nelements, which is composed of three sub-elecments,\\ni.e., <ConceptName>, <ScopeNote> and <TermList>.\\nAmong the sub-elecments, <ScopeNote> is the major\\ntextual information source, which is usually a short de-\\nscription to a medical term or phenomenon. We directly\\nconsider each <ScopeNote> as a document entry and\\nconcatenate it with corresponding <ConceptName>.\\nWe list the statistics of the augmenting corpora in\\nTable 8.\\nA.2\\nBaselines\\nWe use the baselines from the current BEIR leader-\\nboard (Thakur et al., 2021b) and recent papers. These\\nbaselines can be divided into four groups: dense re-', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='Zichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='dataset for medical information retrieval. In Euro-\\npean Conference on Information Retrieval, pages\\n716–722. Springer.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nClaudio Carpineto and Giovanni Romano. 2012. A\\nsurvey of automatic query expansion in information\\nretrieval. Acm Computing Surveys (CSUR), 44(1):1–\\n50.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to Answer Open-\\nDomain Questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 1870–1879.\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\\nDowney, and Daniel Weld. 2020.\\nSPECTER:\\nDocument-level\\nrepresentation\\nlearning\\nusing\\ncitation-informed transformers.\\nIn Proceedings\\nof the 58th Annual Meeting of the Association for', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='ments. For datasets that mainly rely on knowledge\\nfrom Wikipedia, we use the Wikipedia dump from\\nKarpukhin et al. (2020) where articles are divided\\ninto 100-token passages as the document corpus\\nand employ BM25 (Robertson and Zaragoza, 2009)\\nas the retriever. For datasets that rely on knowledge\\nfrom the open web, we use the Bing search engine\\nas our retriever.4\\nRetrieved document formatting.\\nMultiple re-\\ntrieved documents are linearized according to their\\nranking and then added to the beginning of the user\\ninput using the following format:\\nPrompt 3.3: document formatting\\nSearch results:\\n[1] Document 1\\n[2] Document 2\\n...\\nThe user input x\\nEfﬁciency\\nAs shown later in subsection 6.2, on\\naverage retrieval is triggered for 30% ∼ 60% of\\nsentences depending on downstream tasks.\\nIn\\ncomparision, KNN-LM (Khandelwal et al., 2020)\\nretrieves for every token, RETRO or IC-RALM\\n(Borgeaud et al., 2022; Ram et al., 2023) retriev-\\ners every 4∼32 tokens, and IRCoT (Trivedi et al.,', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What are the three sections into which the 14th century work \"The Divine Comedy\" is divided?', 'result': ' The 14th century work \"The Divine Comedy\" is divided into three sections:', 'source_documents': [Document(page_content='Answer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.\\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer (with step-by-step): The ﬁlm Twenty Plus Two was directed by Joseph M. Newman.\\nThe Gal Who Took\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nNow, combine the aforementioned two skills.\\nFirst, write out the reasoning steps, then draw the conclusion,', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='Hint: This question is ambiguous in terms of the time frame or period being referred to.\\nAnswer: In order to ﬁgure out its interpretations,\\nPrompt C.8: exemplars of WikiAsp\\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\\n# Location\\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\\nat 39°56’12\"N 32°51’55\"E.\\n# History\\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸Serafettin after whom the mosque was named.\\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\\nGenerate a summary about Untold Legends:', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='born ﬁrst. So the answer is Martin Hodge.\\nQuestion: When did the director of ﬁlm Laughter In Hell die?\\nAnswer: The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\\nanswer is August 25, 1963.\\nQuestion: Which ﬁlm has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer: The ﬁlm Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nQuestion: Which country the director of ﬁlm Citizen Mavzik is from?\\nAnswer:', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='nationality?\\nAnswer (with step-by-step): Coolie No. 1 (1995 ﬁlm) was directed by David Dhawan. The Sensational Trial was directed by\\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\\nnationality. So the answer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born ﬁrst out of Martin Hodge and Ivania Martinich?\\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\\nMartin Hodge was born ﬁrst. So the answer is Martin Hodge.\\nQuestion: When did the director of ﬁlm Laughter In Hell die?\\nAnswer (with step-by-step): The ﬁlm Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What are the two components of RAG models described in the context?', 'result': ' The two components of RAG models described in the context are the regularized document and attention score ratio', 'source_documents': [Document(page_content='on the four Pile datasets are presented in Table 4.\\nConsistent with the findings on the Wikitext-103\\ndataset, our model demonstrates superior perfor-\\nmance compared to the baseline models across all\\nfour datasets. It is important to note that we did\\nnot finetune the client model specifically on the\\nPile datasets, further highlighting the model’s gen-\\neralization capabilities. Lastly, we have also ob-\\nserved a significantly high perplexity when using\\nthe GPT3 Davinci model. This is due to substan-\\ntial variance in probability between the model’s\\npredictions (generated by GPT3 zero-shot) and the\\nground truth labels (generated by GPT3 with aug-\\nmented memory).\\n4.2.2\\nInference Latency\\nWe performed a latency evaluation for both the\\nOPT-125M and OPT-350M models on the two hard-\\nware setups, as described in Section 4.1.2. Figure\\n5a illustrates that the OPT-125M model exhibits a\\n49.3% faster inference time compared to the OPT-\\n350M model. This finding emphasizes that the size', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='27.1\\n24.5\\n23.3\\n84.4\\nVanilla OPT\\n5.5\\n9.7\\n7.2\\n22.2\\n20.0\\n19.1\\n82.4\\nRAG\\n3.8\\n14.8\\n12.9\\n28.4\\n25.3\\n25.1\\n84.7\\nHybridRAG w/o FT\\n2.8\\n20.1\\n19.9\\n32.9\\n30.4\\n29.7\\n85.1\\nHybridRAG\\n2.8\\n21.7\\n21.1\\n34.6\\n31.8\\n31.9\\n85.9\\nTable 6: Utility Performance of OPT-350M Model on Pile datasets\\nPPL\\nGLEU\\nBLEU-4\\nROUGE-1\\nROUGE-L\\nMETEOR\\nBERTScore\\nEnron\\nEmails\\nGPT3 zero-shot\\n106.9\\n12.3\\n10.4\\n26.1\\n23.3\\n21.6\\n83.6\\nVanilla OPT\\n6.0\\n10.5\\n9.1\\n21.5\\n19.3\\n18.0\\n80.3\\nRAG\\n3.7\\n12.7\\n11.9\\n25.2\\n22.9\\n21.6\\n80.4\\nHybridRAG w/o FT\\n3.2\\n20.3\\n19.9\\n31.0\\n28.6\\n27.4\\n82.7\\nHybridRAG\\n3.7\\n18.9\\n18.9\\n31.6\\n28.3\\n28.2\\n83.8\\nNIH\\nExPorter\\nGPT3 zero-shot\\n12.2\\n18.5\\n16.2\\n36.6\\n31.7\\n29.2\\n86.7\\nVanilla OPT\\n5.4\\n12.0\\n10.8\\n27.9\\n25.3\\n21.4\\n84.3\\nRAG\\n3.8\\n11.5\\n10.5\\n25.4\\n23.3\\n18.6\\n83.8\\nHybridRAG w/o FT\\n2.9\\n19.5\\n19.8\\n33.5\\n31.4\\n27.2\\n85.6\\nHybridRAG\\n3.3\\n23.0\\n23.5\\n36.2\\n33.2\\n30.8\\n85.9\\nHacker\\nNews\\nGPT3 zero-shot\\n65.1\\n15.3\\n14.3\\n30.2\\n27.7\\n20.4\\n85.8\\nVanilla OPT\\n7.8\\n11.8\\n7.5\\n29.7\\n25.2\\n22.2\\n84.7\\nRAG\\n4.7\\n15.8\\n12.8\\n34.1\\n29.1\\n27.2\\n85.8\\nHybridRAG w/o FT\\n3.8\\n19.5\\n17.5\\n37.3\\n32.6\\n30.8\\n86.4\\nHybridRAG', metadata={'Published': '2023-08-08', 'Title': 'Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance', 'Authors': 'Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle', 'Summary': \"Retrieval augmented models show promise in enhancing traditional language\\nmodels by improving their contextual understanding, integrating private data,\\nand reducing hallucination. However, the processing time required for retrieval\\naugmented large language models poses a challenge when applying them to tasks\\nthat require real-time responses, such as composition assistance.\\n  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\\nGeneration (HybridRAG) framework that leverages a hybrid setting that combines\\nboth client and cloud models. HybridRAG incorporates retrieval-augmented memory\\ngenerated asynchronously by a Large Language Model (LLM) in the cloud. By\\nintegrating this retrieval augmented memory, the client model acquires the\\ncapability to generate highly effective responses, benefiting from the LLM's\\ncapabilities. Furthermore, through asynchronous memory integration, the client\\nmodel is capable of delivering real-time responses to user requests without the\\nneed to wait for memory synchronization from the cloud. Our experiments on\\nWikitext and Pile subsets show that HybridRAG achieves lower latency than a\\ncloud-based retrieval-augmented LLM, while outperforming client-only models in\\nutility.\"}), Document(page_content='Epi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n(b) ADist Att. Score.\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(c) MoMA Doc Ratio.\\nEpi-0\\nEpi-1\\nEpi-2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(d) ADist Doc Ratio.\\nFigure 2: Grounding component breakdown for different distillation methods in each learning iteration. We display\\nthe regularized doc and att. score ratio of documents from different augmentation sources.\\nNQ\\nHotpotQA\\nFEVER\\n0\\n20\\n40\\n60\\n80\\n100\\nTarget\\nWiki\\nMeSH\\n(a) Doc Ratio. (Wiki)\\nNFCorpus\\nTREC-Covid\\nBIOASQ\\n0\\n20\\n40\\n60\\n80\\n100\\n(b) Doc Ratio. (Med)\\nNQ\\nHotpotQA\\nFEVER\\n0\\n20\\n40\\n60\\n80\\n100\\n(c) Att. Score Ratio. (Wiki)\\nNFCorpus\\nTREC-Covid\\nBIOASQ\\n0\\n20\\n40\\n60\\n80\\n100\\n(d) Att. Score Ratio. (Med)\\nFigure 3: The inclusion of Plug-In memory during testing (grouped by the Wiki and Medical domains).\\nto MeSH documents, especially on TREC-Covid task\\nsince MeSH includes high quality updated information\\nrelated to COVID-19. Wikipedia documents received\\nmore attention on the Wiki-centric tasks like FEVER, as', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='the search-related instruction and exemplars at the\\nbeginning as skill 1, followed by the instruction and\\nexemplars of the downstream task as skill 2. Given\\na test case, we ask LMs to combine skills 1 and 2 to\\ngenerate search queries while performing the task.\\nThe structure of the prompt is shown in Prompt 3.1,\\nand further details can be found in Prompt C.1.\\nPrompt 3.1: retrieval instructions\\nSkill 1. An instruction to guide LMs to generate search\\nqueries.\\nSeveral search-related exemplars.\\nSkill 2.\\nAn instruction to guide LMs to perform a\\nspeciﬁc downstream task (e.g., multihop QA).\\nSeveral task-related exemplars.\\nAn instruction to guide LMs to combine skills 1\\nand 2 for the test case.\\nThe input of the test case.\\nAs shown in Figure 2, when the LM generates\\n“[Search(query)]” (shown in gray italic), we stop\\nthe generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the title of the paper on Retrieval Augmented Generation?', 'result': ' The title of the paper on Retrieval Augmented Generation is \"Forward-Looking Active', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the benchmark dataset used for question answering research mentioned in the provided context?', 'result': ' The benchmark dataset used for question answering research mentioned in the provided context is MS MARCO.', 'source_documents': [Document(page_content='munity QA6 with 2 billion question-answer pairs then\\nﬁne-tuned on NQ and MS Marco dataset. In addition,\\nthey use the hard negatives released by RocketQA (Qu\\net al., 2021) when ﬁnetuning with MS Marco data and\\nthe hard negatives release by (Lu et al., 2021) for Nat-\\nural Questions. GTRbase leverages the same T5-base\\nmodel as MoMA, while GTRlarge is based on T5-large,\\nwhich is not directly comparable to our method as it\\ntriples the parameters.\\nDense Retrieval with Generated Queries\\nGenQ\\nﬁrst ﬁne-tunes a T5-base (Raffel et al., 2019) model on\\nMS MARCO for 2 epochs and then generate 5 queries\\n6Unfortunately, this corpus has not been released by the\\nauthors.\\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\\npaper (Thakur et al., 2021b).\\nSplit (→)\\nTrain\\nDev\\nTest\\nAvg. Word Lengths\\nTask (↓)\\nDomain (↓)\\nDataset (↓)\\nTitle\\nRelevancy\\n#Pairs\\n#Query\\n#Query\\n#Corpus\\nAvg. D / Q\\nQuery\\nDocument\\nPassage-Retrieval\\nMisc.\\nMS MARCO\\n\\x17\\nBinary\\n532,761\\n—-', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='Zichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='Table 8.\\nA.2\\nBaselines\\nWe use the baselines from the current BEIR leader-\\nboard (Thakur et al., 2021b) and recent papers. These\\nbaselines can be divided into four groups: dense re-\\ntrieval, dense retrieval with generated queries5, lexical\\nretrieval and late interaction.\\n4https://github.com/beir-cellar/beir\\n5We separate them from dense retrieval since they usually\\nrely on Seq2seq models to generate pseudo query-document\\npairs, and they train a model for each dataset independently\\ninstead of using a single model for all datasets.\\nDense Retrieval\\nFor dense retrieval, the baselines\\nare the same dual-tower model as ours. We consider\\nDPR (Karpukhin et al., 2020), ANCE (Xiong et al.,\\n2020), T5-ANCE, coCondenser (Gao and Callan,\\n2022) and one recently-proposed model GTR (Ni et al.,\\n2021) with different size conﬁguration in this paper.\\n• DPR uses a single BM25 retrieval example and in-\\nbatch examples as hard negative examples to train\\nthe model. Different from the original paper (Thakur', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='dataset for medical information retrieval. In Euro-\\npean Conference on Information Retrieval, pages\\n716–722. Springer.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nClaudio Carpineto and Giovanni Romano. 2012. A\\nsurvey of automatic query expansion in information\\nretrieval. Acm Computing Surveys (CSUR), 44(1):1–\\n50.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to Answer Open-\\nDomain Questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 1870–1879.\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\\nDowney, and Daniel Weld. 2020.\\nSPECTER:\\nDocument-level\\nrepresentation\\nlearning\\nusing\\ncitation-informed transformers.\\nIn Proceedings\\nof the 58th Annual Meeting of the Association for', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'})]}\n",
      "{'query': 'What are the two models proposed in the paper on Retrieval Augmented Generation?', 'result': ' The two models proposed in the paper on Retrieval Augmented Generation are FLAREinstruct', 'source_documents': [Document(page_content='throughout the generation process, and are appli-\\ncable to a variety of long-form generation tasks?\\nWe consider a new paradigm, active retrieval aug-\\nmented generation. Our hypothesis regarding when\\nto retrieve is that LMs should retrieve information\\nonly when they lack the required knowledge to\\navoid unnecessary or inappropriate retrieval that\\noccurs in passive retrieval-augmented LMs (Khan-\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\net al., 2023; Trivedi et al., 2022). Given the obser-\\nvation that large LMs tend to be well-calibrated and\\nlow probability/conﬁdence often indicates a lack\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022), we adopt an active retrieval strategy that\\nonly retrieves when LMs generate low-probability\\ntokens. When deciding what to retrieve, we argue\\nthat it is important to consider what LMs intend to\\ngenerate in the future, as the goal of active retrieval\\nis to beneﬁt future generations. Therefore, we pro-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='denoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-\\nates the next sentence to gain insight into the future\\ntopic, and if uncertain tokens are present, retrieves\\nrelevant documents to regenerate the next sentence.\\n3.1\\nFLARE with Retrieval Instructions\\nA straightforward way of expressing information\\nneeds for retrieval is to generate “[Search(query)]”\\nwhen additional information is needed (Schick\\net al., 2023), e.g., “The colors on the ﬂag of\\nGhana have the following meanings. Red is for\\n[Search(Ghana ﬂag red meaning)] the blood of mar-\\ntyrs, ...” When working with GPT-3.5 models that\\noffer only API access, we elicit such behavior by\\nfew-shot prompting (Brown et al., 2020).\\nSpeciﬁcally, for a downstream task, we place\\nthe search-related instruction and exemplars at the\\nbeginning as skill 1, followed by the instruction and\\nexemplars of the downstream task as skill 2. Given', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the approach used to train the retriever and generator components in the paper on Retrieval Augmented Generation?', 'result': ' The retriever and generator components in the paper on Retrieval Augmented Generation are trained', 'source_documents': [Document(page_content='boost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the best performing \"closed-book\" open-domain QA model mentioned in the context?', 'result': ' The best performing \"closed-book\" open-domain QA model mentioned in the context is FL', 'source_documents': [Document(page_content='munity QA6 with 2 billion question-answer pairs then\\nﬁne-tuned on NQ and MS Marco dataset. In addition,\\nthey use the hard negatives released by RocketQA (Qu\\net al., 2021) when ﬁnetuning with MS Marco data and\\nthe hard negatives release by (Lu et al., 2021) for Nat-\\nural Questions. GTRbase leverages the same T5-base\\nmodel as MoMA, while GTRlarge is based on T5-large,\\nwhich is not directly comparable to our method as it\\ntriples the parameters.\\nDense Retrieval with Generated Queries\\nGenQ\\nﬁrst ﬁne-tunes a T5-base (Raffel et al., 2019) model on\\nMS MARCO for 2 epochs and then generate 5 queries\\n6Unfortunately, this corpus has not been released by the\\nauthors.\\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\\npaper (Thakur et al., 2021b).\\nSplit (→)\\nTrain\\nDev\\nTest\\nAvg. Word Lengths\\nTask (↓)\\nDomain (↓)\\nDataset (↓)\\nTitle\\nRelevancy\\n#Pairs\\n#Query\\n#Query\\n#Corpus\\nAvg. D / Q\\nQuery\\nDocument\\nPassage-Retrieval\\nMisc.\\nMS MARCO\\n\\x17\\nBinary\\n532,761\\n—-', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='Among the 4 following options, which is\\nthe correct answer?\\n- A: {choice_A}\\n- B: {choice_B}\\n- C: {choice_C}\\n- D: {choice_D}\\nThe prompt template for PopQA is:\\nQ: {question} A:\\nB\\nSelection of Source Task\\nWe provide a detailed selection of the source tasks\\nhere, using a variety of source and target tasks to an-\\nalyze. MSMARCO QA, KILT-TriviaQA, and NQ\\nbelong to Open Domain QA, while KILT-T-REx\\nand zsRE belong to Slot Filling. MMLU belongs\\nto Multi-task Language Understanding, which is\\n3https://huggingface.co/OpenMatch/t5-ance\\n4https://huggingface.co/facebook/contriever-msmarco\\nTs\\nTt\\nMMLU\\nNQ\\nzsRE\\nMSMARCO QA\\n44.8\\n46.7\\n75.1\\nKILT-TriviaQA\\n43.6\\n46.4\\n74.9\\nKILT-T-REx\\n44.1\\n45.9\\n77.2\\nTable 5: Relationship between the selection of source\\ntask Ts and the performance of target task Tt. The\\nmodel is Flan-T5Base w/ AARANCE. As NQ and zsRE\\nare included in the Flan-T5 training data, we only report\\ntheir F1 results here for reference.\\ncloser to the Open Domain QA in terms of the task', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='and compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.\\n6.1\\nComparison with Baselines\\nOverall results.\\nThe overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks and datasets, multihop QA\\nshows the most signiﬁcant improvement. This is\\nlargely due to the task’s clear deﬁnition and speciﬁc\\nobjective of producing the ﬁnal answer through a\\n2-hop reasoning process, which makes it easier\\nfor LMs to generate on-topic output. In contrast,\\nASQA and WikiAsp are less clearly deﬁned and\\nmore open-ended, which increases the difﬁculty of\\n6To avoid leaking, we exclude several Wikipedia-related', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='model is Flan-T5Base w/ AARANCE. As NQ and zsRE\\nare included in the Flan-T5 training data, we only report\\ntheir F1 results here for reference.\\ncloser to the Open Domain QA in terms of the task\\nobjective. As shown in Table 5, when we align the\\ncategory of the source task with the target task, the\\nLM w/ AAR can generally achieve the best results.\\nWe suppose that this is because LM may share sim-\\nilar document preferences on the tasks from the\\nsame dataset category, making AAR easier to gen-\\neralize. Furthermore, taking MSMARCO QA as\\nthe source task performs the best on MMLU. This\\nvalidates the rationality to set Ts as MSMARCO\\nQA in our main experimental settings.\\nC\\nAAR’s Improvements on PopQA\\n250M\\n780M\\n3B\\n175B\\n# Parameters\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nPopQA Accuracy\\nANCE\\nAARANCE\\nContriever\\nAARContriever\\nFigure 8: AAR’s improvements on PopQA, using Flan-\\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\\nInstructGPT (175B) as target LMs.\\nD\\nFine-tuning Results', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"})]}\n",
      "{'query': 'What is the ratio of distinct to total tri-grams for the generation tasks in the Jeopardy Question Generation Task?', 'result': ' The ratio of distinct to total trigrams for the generation tasks in the Jeopardy', 'source_documents': [Document(page_content='erates questions for low-conﬁdence spans without\\nadditional annotation. Speciﬁcally, We ﬁrst extract\\nall spans from ˆst with probabilities below β. For\\neach extracted span z, we prompt gpt-3.5-turbo\\nto generate a question qt,z that can be answered\\nwith the span, using the following prompt:\\nPrompt 3.2: zero-shot question generation\\nUser input x.\\nGenerated output so far y≤t.\\nGiven the above passage, ask a question to which\\nthe answer is the term/entity/phrase “z”.\\nWe retrieve using each generated question and\\ninterleave the returned documents into a single\\nranking list to aid future generations. In summary,\\nqueries qt are formulated based on ˆst as follows:\\nqt =\\n�\\n∅\\nif all tokens of ˆst have probs ≥ θ\\nmask(ˆst) or qgen(ˆst)\\notherwise\\n3.3\\nImplementation Details\\nWe validate our method using one of the most ad-\\nvanced GPT-3.5 LMs text-davinci-003 by itera-\\ntively querying their API.2\\n2https://api.openai.com/v1/completions in April\\n2023.\\nThe initial query.\\nFLARE starts with the user', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='and compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.\\n6.1\\nComparison with Baselines\\nOverall results.\\nThe overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks and datasets, multihop QA\\nshows the most signiﬁcant improvement. This is\\nlargely due to the task’s clear deﬁnition and speciﬁc\\nobjective of producing the ﬁnal answer through a\\n2-hop reasoning process, which makes it easier\\nfor LMs to generate on-topic output. In contrast,\\nASQA and WikiAsp are less clearly deﬁned and\\nmore open-ended, which increases the difﬁculty of\\n6To avoid leaking, we exclude several Wikipedia-related', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='lected based on the development set and listed in\\nTable 8. FLARE refers to FLAREdirect if not specif-\\nically stated. For previous-window approaches,\\nwe follow Ram et al. (2023) to use a window size\\nl = 16 in our experiments.\\n5.1\\nMultihop QA\\nDataset\\nThe goal of multihop QA is to answer\\ncomplex questions through a process of informa-\\ntion retrieval and reasoning (Yang et al., 2018; Ho\\net al., 2020). For instance, to answer “Why did\\nthe founder of Versus die?”, we must ﬁrst identify\\nwho founded Versus and subsequently determine\\nthe cause of their death. Multihop QA also uniﬁes\\ninto long-form generation when solved with the\\nstate-of-the-art CoT methods (Wei et al., 2022).\\nWe use 2WikiMultihopQA (Ho et al., 2020)\\nwhich contains 2-hop complex questions sourced\\nfrom Wikipedia articles that require composition,\\ncomparison, or inference.\\nSettings\\nWe follow Wang et al. (2022) to gen-\\nerate both the chain-of-thought reasoning process\\nand the ﬁnal answer. For the above case, the output', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the main finding of the paper on Retrieval Augmented Generation?', 'result': ' The main finding of the paper on Retrieval Augmented Generation is that LMs can effectively', 'source_documents': [Document(page_content='resulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ..., yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe start of the generation (t = 1), the previous\\ngeneration is empty (y<1 = ∅), and the user input\\nis used as the initial query (q1 = x). Given the re-\\ntrieved documents Dqt, LMs continually generate\\nthe answer until the next retrieval is triggered or\\nreaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the\\ncurrent step t, and the input to LMs is the concate-\\nnation of the retrieved documents Dqt, the user\\ninput x, and the previous generation y<t. At each\\nstep, we discard previously retrieved documents\\n∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='throughout the generation process, and are appli-\\ncable to a variety of long-form generation tasks?\\nWe consider a new paradigm, active retrieval aug-\\nmented generation. Our hypothesis regarding when\\nto retrieve is that LMs should retrieve information\\nonly when they lack the required knowledge to\\navoid unnecessary or inappropriate retrieval that\\noccurs in passive retrieval-augmented LMs (Khan-\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\net al., 2023; Trivedi et al., 2022). Given the obser-\\nvation that large LMs tend to be well-calibrated and\\nlow probability/conﬁdence often indicates a lack\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022), we adopt an active retrieval strategy that\\nonly retrieves when LMs generate low-probability\\ntokens. When deciding what to retrieve, we argue\\nthat it is important to consider what LMs intend to\\ngenerate in the future, as the goal of active retrieval\\nis to beneﬁt future generations. Therefore, we pro-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the main objective of the work presented in the paper on Retrieval Augmented Generation?', 'result': ' The main objective of the work presented in the paper on Retrieval Augmented Generation is to', 'source_documents': [Document(page_content='∪t′<tDqt′ and only use the retrieved documents\\nfrom the current step to condition the next gener-\\nation to prevent reaching the input length limit of\\nLMs.\\n3\\nFLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should re-\\nﬂect the intents of future generations. Therefore,\\nWe propose two forward-looking active retrieval\\naugmented generation (FLARE) methods to im-\\nplement the active retrieval augmented generation\\nframework. Inspired by Toolformer (Schick et al.,\\n2023), the ﬁrst method prompts the LM to generate\\nretrieval queries when necessary while generating\\nthe answer using retrieval-encouraging instructions,\\ndenoted as FLAREinstruct. The second method di-\\nrectly uses the LM’s generation as search queries,\\ndenoted as FLAREdirect, which iteratively gener-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='throughout the generation process, and are appli-\\ncable to a variety of long-form generation tasks?\\nWe consider a new paradigm, active retrieval aug-\\nmented generation. Our hypothesis regarding when\\nto retrieve is that LMs should retrieve information\\nonly when they lack the required knowledge to\\navoid unnecessary or inappropriate retrieval that\\noccurs in passive retrieval-augmented LMs (Khan-\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\net al., 2023; Trivedi et al., 2022). Given the obser-\\nvation that large LMs tend to be well-calibrated and\\nlow probability/conﬁdence often indicates a lack\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022), we adopt an active retrieval strategy that\\nonly retrieves when LMs generate low-probability\\ntokens. When deciding what to retrieve, we argue\\nthat it is important to consider what LMs intend to\\ngenerate in the future, as the goal of active retrieval\\nis to beneﬁt future generations. Therefore, we pro-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='the user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the speciﬁed order.\\n2.2\\nSingle-time Retrieval-Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]) (Chen\\net al., 2017; Guu et al., 2020; Lewis et al., 2020;\\nIzacard and Grave, 2021; Sachan et al., 2021; Lee\\net al., 2021; Jiang et al., 2022; Izacard et al., 2022;\\nShi et al., 2023).\\n2.3\\nActive Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What are the limitations of large pre-trained language models in accessing and manipulating knowledge in knowledge-intensive tasks?', 'result': ' Large pre-trained language models have limitations in accessing and manipulating knowledge in knowledge-intensive', 'source_documents': [Document(page_content='Michael Zeng, and Meng Jiang. 2023.\\nGenerate\\nrather than retrieve: Large language models are\\nstrong context generators. In ICLR.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\\ntrained transformer language models. arXiv preprint\\narXiv:2205.01068.\\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong\\nWang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan\\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.\\nPlug-and-play knowledge injection for pre-trained\\nlanguage models. In Proceedings of ACL.\\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\\nand Lichao Sun. 2023. A comprehensive survey on', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='et al., 2019; Brown et al., 2020; Smith et al., 2022), but\\nwith diminishing return: linear improvements on down-\\nstream metrics often require exponentially more parame-\\nters and computing cost (Kaplan et al., 2020; Hoffmann\\net al., 2022). Hence, scaling pretrained language mod-\\nels in this way is economically unsustainable (Strubell\\net al., 2020; Bender et al., 2021; Zhang et al., 2022).\\nRetrieval augmented language models provide a\\npromising alternative. They allow language models\\nto efﬁciently access vast resources from an external cor-\\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\\nas a kind of “memory” they can refer to when making\\npredictions, alleviating the need to memorize as much\\n∗Work partly done during Suyu’s internship at Microsoft.\\ninformation in their own network parameters (Roberts\\net al., 2020). This open-book approach helps language\\nmodels to better generalize on token prediction tasks and\\nmachine translation (Khandelwal et al., 2019; Borgeaud', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='Kilt: a benchmark for knowl-\\nedge intensive language tasks.\\narXiv preprint\\narXiv:2009.02252.\\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\\nand Haifeng Wang. 2021. RocketQA: An optimized\\ntraining approach to dense passage retrieval for open-\\ndomain question answering. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, pages 5835–5847, On-\\nline. Association for Computational Linguistics.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2019. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. Journal of Machine Learning Research.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the parame-\\nters of a language model? In EMNLP.\\nStephen Robertson, Hugo Zaragoza, et al. 2009. The', metadata={'Published': '2023-02-07', 'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories', 'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett', 'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'}), Document(page_content='Riedel. KILT: a benchmark for knowledge intensive\\nlanguage tasks. In Proceedings of NAACL, pages\\n2523–2544.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. JMLR, 21:140:1–140:67.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the parame-\\nters of a language model? In Proceedings of EMNLP,\\npages 5418–5426.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2022. Learning to retrieve prompts for in-context\\nlearning. In Proceedings of NAACL, pages 2655–\\n2671.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.\\nMultitask prompted training enables zero-shot task\\ngeneralization. In ICLR.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"})]}\n",
      "{'query': 'What is the main objective of RAG in the experiments conducted in the paper on Retrieval Augmented Generation?', 'result': ' The main objective of RAG in the experiments conducted in the paper on Retrieval Augmented', 'source_documents': [Document(page_content='Zichun Yu conducted the experiments. Zichun Yu,\\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\\nvided valuable suggestions for the research. We\\nthank Suyu Ge for sharing the ANCE checkpoint\\ninitialized from T5Base.\\nReferences\\nFabien André, Anne-Marie Kermarrec, and Nicolas\\nLe Scouarnec. 2016. Cache locality is not enough:\\nHigh-performance nearest neighbor search with prod-\\nuct quantization fast scan. In VLDB, page 12.\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\\nwith instructions. arXiv preprint arXiv:2211.09260.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\\n2016. Ms marco: A human generated machine read-\\ning comprehension dataset. In CoCo@NeurIPS.\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='throughout the generation process, and are appli-\\ncable to a variety of long-form generation tasks?\\nWe consider a new paradigm, active retrieval aug-\\nmented generation. Our hypothesis regarding when\\nto retrieve is that LMs should retrieve information\\nonly when they lack the required knowledge to\\navoid unnecessary or inappropriate retrieval that\\noccurs in passive retrieval-augmented LMs (Khan-\\ndelwal et al., 2020; Borgeaud et al., 2022; Ram\\net al., 2023; Trivedi et al., 2022). Given the obser-\\nvation that large LMs tend to be well-calibrated and\\nlow probability/conﬁdence often indicates a lack\\nof knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022), we adopt an active retrieval strategy that\\nonly retrieves when LMs generate low-probability\\ntokens. When deciding what to retrieve, we argue\\nthat it is important to consider what LMs intend to\\ngenerate in the future, as the goal of active retrieval\\nis to beneﬁt future generations. Therefore, we pro-', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='boost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4\\nIntegration\\nData Augmentation\\nThere are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation, which constructs some augmented inputs\\nby concatenating spans from {⟨xr, yr⟩} with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).\\nAttention\\nMechanisms\\nAnother\\nintegration\\nmethod\\nis\\nbased\\non\\nattention\\nmechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}), Document(page_content='the generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the next\\nsearch query is generated or reaches the end.\\nSearch results:   !!\\n[1]: …\\n[2]: …\\nJoe Biden attended\\nSearch results:   !\"!\\n[1]: …\\n[2]: …Search results:   !\"\"\\n[1]: …\\n[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]\\nthe University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.\\nInput\\n$\\n&$\\n&#\\n%$\\n&%\\n%%\\nGeneration\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information\\nto aid future generations.\\nWe found that LMs can effectively combine the\\ntwo skills and generate meaningful search queries\\nwhile performing the task. However, there are\\ntwo issues: (1) LMs tend to generate fewer search', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n",
      "{'query': 'What is the purpose of the MSMARCO NLG task v2.1?', 'result': ' The MSMARCO NLG task v2.1 is a natural language generation task that aims', 'source_documents': [Document(page_content='model is Flan-T5Base w/ AARANCE. As NQ and zsRE\\nare included in the Flan-T5 training data, we only report\\ntheir F1 results here for reference.\\ncloser to the Open Domain QA in terms of the task\\nobjective. As shown in Table 5, when we align the\\ncategory of the source task with the target task, the\\nLM w/ AAR can generally achieve the best results.\\nWe suppose that this is because LM may share sim-\\nilar document preferences on the tasks from the\\nsame dataset category, making AAR easier to gen-\\neralize. Furthermore, taking MSMARCO QA as\\nthe source task performs the best on MMLU. This\\nvalidates the rationality to set Ts as MSMARCO\\nQA in our main experimental settings.\\nC\\nAAR’s Improvements on PopQA\\n250M\\n780M\\n3B\\n175B\\n# Parameters\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\nPopQA Accuracy\\nANCE\\nAARANCE\\nContriever\\nAARContriever\\nFigure 8: AAR’s improvements on PopQA, using Flan-\\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\\nInstructGPT (175B) as target LMs.\\nD\\nFine-tuning Results', metadata={'Published': '2023-05-27', 'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In', 'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu', 'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}), Document(page_content='sponding metrics. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\\n2WikiMultihopQA\\nASQA-hint\\nEM\\nF1\\nPrec. Rec.\\nEM D-F1 R-L DR\\nPrevious 39.0 49.2 48.9 51.8\\n42.5 34.1 36.9 35.5\\nNext\\n48.8 57.6 57.1 60.5\\n45.9 35.7 37.5 36.6\\nTable 3: A head-to-head comparison between using the\\nprevious sentence and the next sentence for retrieval.\\nImportance of active retrieval.\\nNext, we inves-\\ntigate the relationship between performance and\\nthe active retrieval threshold θ. To alter our method\\nfrom not retrieving anything to retrieving every sen-\\ntence, we adjusted the conﬁdence threshold θ used\\nto determine when to trigger retrieval from 0 to\\n1. We calculate the percentage of steps/sentences\\nwhere retrieval is triggered for every threshold and\\ndisplay the performance based on the percentage\\nof retrieval. As shown in Figure 5, on 2WikiMul-\\ntihopQA, the performance plateaus when the re-\\ntrieval percentage exceeds 60%, indicating that re-\\ntrieval when LMs are conﬁdent is not necessary.', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='and compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efﬁcacy of\\nvarious design choices of our method.\\n6.1\\nComparison with Baselines\\nOverall results.\\nThe overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks and datasets, multihop QA\\nshows the most signiﬁcant improvement. This is\\nlargely due to the task’s clear deﬁnition and speciﬁc\\nobjective of producing the ﬁnal answer through a\\n2-hop reasoning process, which makes it easier\\nfor LMs to generate on-topic output. In contrast,\\nASQA and WikiAsp are less clearly deﬁned and\\nmore open-ended, which increases the difﬁculty of\\n6To avoid leaking, we exclude several Wikipedia-related', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'}), Document(page_content='based summarization. Trans. Assoc. Comput. Lin-\\nguistics, 9:211–225.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy\\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\nhardt. 2020. Measuring massive multitask language\\nunderstanding. CoRR, abs/2009.03300.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International\\nConference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,\\n2020, pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, EACL 2021, Online, April 19 - 23, 2021, pages\\n874–880. Association for Computational Linguis-\\ntics.', metadata={'Published': '2023-05-11', 'Title': 'Active Retrieval Augmented Generation', 'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig', 'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing\\nretrieval-augmented LMs employ a retrieve-and-generate setup that only\\nretrieves information once based on the input. This is limiting, however, in\\nmore general scenarios involving generation of long texts, where continually\\ngathering information throughout the generation process is essential. There\\nhave been some past efforts to retrieve information multiple times while\\ngenerating outputs, which mostly retrieve documents at fixed intervals using\\nthe previous context as queries. In this work, we provide a generalized view of\\nactive retrieval augmented generation, methods that actively decide when and\\nwhat to retrieve across the course of the generation. We propose\\nForward-Looking Active REtrieval augmented generation (FLARE), a generic\\nretrieval-augmented generation method which iteratively uses a prediction of\\nthe upcoming sentence to anticipate future content, which is then utilized as a\\nquery to retrieve relevant documents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along with baselines comprehensively over\\n4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\\nsuperior or competitive performance on all tasks, demonstrating the\\neffectiveness of our method. Code and datasets are available at\\nhttps://github.com/jzbjyb/FLARE.'})]}\n"
     ]
    }
   ],
   "source": [
    "question_context_answer = []\n",
    "\n",
    "for idx, question in enumerate(question_list):\n",
    "    result = test_pipeline({\"query\":question})\n",
    "    # print(result)\n",
    "    answer_package = {\n",
    "        \"question\":question,\n",
    "        \"contexts\":[page.page_content  for page in result[\"source_documents\"]],\n",
    "        \"answer\":result[\"result\"]\n",
    "    }\n",
    "\n",
    "    question_context_answer.append(answer_package)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert these responses into a dataset in a format that is compatible with RAGAS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=question_context_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[the generation and use the query terms to ret...</td>\n",
       "      <td>FLARE: Forward-Looking Active REtrieval Augme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[the generation and use the query terms to ret...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the task of the Retrieval Augmented Ge...</td>\n",
       "      <td>[boost the quality of ﬁnal generation. To this...</td>\n",
       "      <td>The task of the Retrieval Augmented Generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What advantages does RAG have over the DPR QA ...</td>\n",
       "      <td>[and compare the performance of FLARE with all...</td>\n",
       "      <td>RAG has several advantages over the DPR QA sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are some potential downsides of using RAG...</td>\n",
       "      <td>[Gretchen Krueger, Tom Henighan, Rewon Child,\\...</td>\n",
       "      <td>\\n\\nThere are several potential downsides of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the file name of the paper on Retrieva...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The file name of the paper on Retrieval Augme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the accuracy of RAG models in generati...</td>\n",
       "      <td>[LM (Ans-Deletion)\\n(b) Answer-deletion test.\\...</td>\n",
       "      <td>The provided context does not mention the acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Question: What is an example of how parametric...</td>\n",
       "      <td>[that our model consistently outperforms all t...</td>\n",
       "      <td>In the context of BART, parametric memory ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the purpose of using multiple answer a...</td>\n",
       "      <td>[lected based on the development set and liste...</td>\n",
       "      <td>Multiple answer annotations are used in open-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Question: Which novel by Ernest Hemingway is b...</td>\n",
       "      <td>[January 22, 2018. (2) Summer of ’84 was relea...</td>\n",
       "      <td>The novel “For Whom the Bell Tolls” is based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What advantage do non-parametric memory models...</td>\n",
       "      <td>[lions of parameters are able to capture a sig...</td>\n",
       "      <td>Non-parametric memory models like RAG have th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What training setup details are mentioned in t...</td>\n",
       "      <td>[Zichun Yu conducted the experiments. Zichun Y...</td>\n",
       "      <td>The paper on Retrieval Augmented Generation m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[the generation and use the query terms to ret...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the title of the paper mentioned in th...</td>\n",
       "      <td>[FEVER\\n(Thorne\\net\\nal.,\\n2018),\\nand\\nClimat...</td>\n",
       "      <td>The title of the paper mentioned in the conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are the three sections into which the 14t...</td>\n",
       "      <td>[Answer (with step-by-step): The ﬁlm Laughter ...</td>\n",
       "      <td>The 14th century work \"The Divine Comedy\" is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What are the two components of RAG models desc...</td>\n",
       "      <td>[on the four Pile datasets are presented in Ta...</td>\n",
       "      <td>The two components of RAG models described in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the title of the paper on Retrieval Au...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The title of the paper on Retrieval Augmented...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the benchmark dataset used for questio...</td>\n",
       "      <td>[munity QA6 with 2 billion question-answer pai...</td>\n",
       "      <td>The benchmark dataset used for question answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What are the two models proposed in the paper ...</td>\n",
       "      <td>[throughout the generation process, and are ap...</td>\n",
       "      <td>The two models proposed in the paper on Retri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the approach used to train the retriev...</td>\n",
       "      <td>[boost the quality of ﬁnal generation. To this...</td>\n",
       "      <td>The retriever and generator components in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the best performing \"closed-book\" open...</td>\n",
       "      <td>[munity QA6 with 2 billion question-answer pai...</td>\n",
       "      <td>The best performing \"closed-book\" open-domain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the ratio of distinct to total tri-gra...</td>\n",
       "      <td>[erates questions for low-conﬁdence spans with...</td>\n",
       "      <td>The ratio of distinct to total trigrams for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What is the main finding of the paper on Retri...</td>\n",
       "      <td>[resulting in the interleaving of retrieval an...</td>\n",
       "      <td>The main finding of the paper on Retrieval Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the main objective of the work present...</td>\n",
       "      <td>[∪t′&lt;tDqt′ and only use the retrieved document...</td>\n",
       "      <td>The main objective of the work presented in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What are the limitations of large pre-trained ...</td>\n",
       "      <td>[Michael Zeng, and Meng Jiang. 2023.\\nGenerate...</td>\n",
       "      <td>Large pre-trained language models have limita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What is the main objective of RAG in the exper...</td>\n",
       "      <td>[Zichun Yu conducted the experiments. Zichun Y...</td>\n",
       "      <td>The main objective of RAG in the experiments ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What is the purpose of the MSMARCO NLG task v2.1?</td>\n",
       "      <td>[model is Flan-T5Base w/ AARANCE. As NQ and zs...</td>\n",
       "      <td>The MSMARCO NLG task v2.1 is a natural langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the title of the paper on Retrieval Au...   \n",
       "1   What is the title of the paper on Retrieval Au...   \n",
       "2   What is the title of the paper on Retrieval Au...   \n",
       "3   What is the title of the paper on Retrieval Au...   \n",
       "4   What is the task of the Retrieval Augmented Ge...   \n",
       "5   What advantages does RAG have over the DPR QA ...   \n",
       "6   What are some potential downsides of using RAG...   \n",
       "7   What is the file name of the paper on Retrieva...   \n",
       "8   What is the accuracy of RAG models in generati...   \n",
       "9   Question: What is an example of how parametric...   \n",
       "10  What is the title of the paper on Retrieval Au...   \n",
       "11  What is the purpose of using multiple answer a...   \n",
       "12  Question: Which novel by Ernest Hemingway is b...   \n",
       "13  What advantage do non-parametric memory models...   \n",
       "14  What is the title of the paper on Retrieval Au...   \n",
       "15  What is the title of the paper on Retrieval Au...   \n",
       "16  What training setup details are mentioned in t...   \n",
       "17  What is the title of the paper on Retrieval Au...   \n",
       "18  What is the title of the paper mentioned in th...   \n",
       "19  What are the three sections into which the 14t...   \n",
       "20  What are the two components of RAG models desc...   \n",
       "21  What is the title of the paper on Retrieval Au...   \n",
       "22  What is the benchmark dataset used for questio...   \n",
       "23  What are the two models proposed in the paper ...   \n",
       "24  What is the approach used to train the retriev...   \n",
       "25  What is the best performing \"closed-book\" open...   \n",
       "26  What is the ratio of distinct to total tri-gra...   \n",
       "27  What is the main finding of the paper on Retri...   \n",
       "28  What is the main objective of the work present...   \n",
       "29  What are the limitations of large pre-trained ...   \n",
       "30  What is the main objective of RAG in the exper...   \n",
       "31  What is the purpose of the MSMARCO NLG task v2.1?   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [resulting in the interleaving of retrieval an...   \n",
       "1   [the generation and use the query terms to ret...   \n",
       "2   [the generation and use the query terms to ret...   \n",
       "3   [resulting in the interleaving of retrieval an...   \n",
       "4   [boost the quality of ﬁnal generation. To this...   \n",
       "5   [and compare the performance of FLARE with all...   \n",
       "6   [Gretchen Krueger, Tom Henighan, Rewon Child,\\...   \n",
       "7   [resulting in the interleaving of retrieval an...   \n",
       "8   [LM (Ans-Deletion)\\n(b) Answer-deletion test.\\...   \n",
       "9   [that our model consistently outperforms all t...   \n",
       "10  [resulting in the interleaving of retrieval an...   \n",
       "11  [lected based on the development set and liste...   \n",
       "12  [January 22, 2018. (2) Summer of ’84 was relea...   \n",
       "13  [lions of parameters are able to capture a sig...   \n",
       "14  [resulting in the interleaving of retrieval an...   \n",
       "15  [resulting in the interleaving of retrieval an...   \n",
       "16  [Zichun Yu conducted the experiments. Zichun Y...   \n",
       "17  [the generation and use the query terms to ret...   \n",
       "18  [FEVER\\n(Thorne\\net\\nal.,\\n2018),\\nand\\nClimat...   \n",
       "19  [Answer (with step-by-step): The ﬁlm Laughter ...   \n",
       "20  [on the four Pile datasets are presented in Ta...   \n",
       "21  [resulting in the interleaving of retrieval an...   \n",
       "22  [munity QA6 with 2 billion question-answer pai...   \n",
       "23  [throughout the generation process, and are ap...   \n",
       "24  [boost the quality of ﬁnal generation. To this...   \n",
       "25  [munity QA6 with 2 billion question-answer pai...   \n",
       "26  [erates questions for low-conﬁdence spans with...   \n",
       "27  [resulting in the interleaving of retrieval an...   \n",
       "28  [∪t′<tDqt′ and only use the retrieved document...   \n",
       "29  [Michael Zeng, and Meng Jiang. 2023.\\nGenerate...   \n",
       "30  [Zichun Yu conducted the experiments. Zichun Y...   \n",
       "31  [model is Flan-T5Base w/ AARANCE. As NQ and zs...   \n",
       "\n",
       "                                               answer  \n",
       "0    The title of the paper on Retrieval Augmented...  \n",
       "1    FLARE: Forward-Looking Active REtrieval Augme...  \n",
       "2    The title of the paper on Retrieval Augmented...  \n",
       "3    The title of the paper on Retrieval Augmented...  \n",
       "4    The task of the Retrieval Augmented Generatio...  \n",
       "5    RAG has several advantages over the DPR QA sy...  \n",
       "6   \\n\\nThere are several potential downsides of u...  \n",
       "7    The file name of the paper on Retrieval Augme...  \n",
       "8    The provided context does not mention the acc...  \n",
       "9    In the context of BART, parametric memory ref...  \n",
       "10   The title of the paper on Retrieval Augmented...  \n",
       "11   Multiple answer annotations are used in open-...  \n",
       "12   The novel “For Whom the Bell Tolls” is based ...  \n",
       "13   Non-parametric memory models like RAG have th...  \n",
       "14   The title of the paper on Retrieval Augmented...  \n",
       "15   The title of the paper on Retrieval Augmented...  \n",
       "16   The paper on Retrieval Augmented Generation m...  \n",
       "17   The title of the paper on Retrieval Augmented...  \n",
       "18   The title of the paper mentioned in the conte...  \n",
       "19   The 14th century work \"The Divine Comedy\" is ...  \n",
       "20   The two components of RAG models described in...  \n",
       "21   The title of the paper on Retrieval Augmented...  \n",
       "22   The benchmark dataset used for question answe...  \n",
       "23   The two models proposed in the paper on Retri...  \n",
       "24   The retriever and generator components in the...  \n",
       "25   The best performing \"closed-book\" open-domain...  \n",
       "26   The ratio of distinct to total trigrams for t...  \n",
       "27   The main finding of the paper on Retrieval Au...  \n",
       "28   The main objective of the work presented in t...  \n",
       "29   Large pre-trained language models have limita...  \n",
       "30   The main objective of RAG in the experiments ...  \n",
       "31   The MSMARCO NLG task v2.1 is a natural langua...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=question_context_answer)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'answer'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d08164d0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0816bc0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0815ff0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d08160b0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0817df0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\")).\n",
      "  0%|          | 0/3 [00:30<?, ?it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Skipping trace saving - unable to safely convert LangChain Run into W&B Trace due to: 'NoneType' object has no attribute 'items'\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0816ef0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m LocationParseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[39m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[39mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m-> 1092\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    612\u001b[0m server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x2d0816ef0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:874\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 874\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    875\u001b[0m         method,\n\u001b[1;32m    876\u001b[0m         url,\n\u001b[1;32m    877\u001b[0m         body,\n\u001b[1;32m    878\u001b[0m         headers,\n\u001b[1;32m    879\u001b[0m         retries,\n\u001b[1;32m    880\u001b[0m         redirect,\n\u001b[1;32m    881\u001b[0m         assert_same_host,\n\u001b[1;32m    882\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    883\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    884\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    885\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    886\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    887\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    888\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    889\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    892\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:874\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 874\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    875\u001b[0m         method,\n\u001b[1;32m    876\u001b[0m         url,\n\u001b[1;32m    877\u001b[0m         body,\n\u001b[1;32m    878\u001b[0m         headers,\n\u001b[1;32m    879\u001b[0m         retries,\n\u001b[1;32m    880\u001b[0m         redirect,\n\u001b[1;32m    881\u001b[0m         assert_same_host,\n\u001b[1;32m    882\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    883\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    884\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    885\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    886\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    887\u001b[0m         preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    888\u001b[0m         decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    889\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    892\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0816ef0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0816ef0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mragas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcritique\u001b[39;00m \u001b[39mimport\u001b[39;00m harmfulness\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mragas\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate\n\u001b[0;32m----> 9\u001b[0m result \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     10\u001b[0m     dataset,\n\u001b[1;32m     11\u001b[0m     metrics\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     12\u001b[0m         answer_relevancy,\n\u001b[1;32m     13\u001b[0m         faithfulness,\n\u001b[1;32m     14\u001b[0m         context_precision,\n\u001b[1;32m     15\u001b[0m         harmfulness\n\u001b[1;32m     16\u001b[0m     ],\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/ragas/evaluation.py:105\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, column_map)\u001b[0m\n\u001b[1;32m    103\u001b[0m         binary_metrics\u001b[39m.\u001b[39mappend(metric\u001b[39m.\u001b[39mname)\n\u001b[1;32m    104\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mevaluating with [\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     scores\u001b[39m.\u001b[39mappend(metric\u001b[39m.\u001b[39;49mscore(dataset)\u001b[39m.\u001b[39mselect_columns(metric\u001b[39m.\u001b[39mname))\n\u001b[1;32m    107\u001b[0m \u001b[39m# log the evaluation event\u001b[39;00m\n\u001b[1;32m    108\u001b[0m metrics_names \u001b[39m=\u001b[39m [m\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metrics]\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/ragas/metrics/base.py:74\u001b[0m, in \u001b[0;36mMetric.score\u001b[0;34m(self, dataset, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mwith\u001b[39;00m trace_as_chain_group(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mragas_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, callback_manager\u001b[39m=\u001b[39mcm) \u001b[39mas\u001b[39;00m group:\n\u001b[1;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_batches(\u001b[39mlen\u001b[39m(dataset))):\n\u001b[0;32m---> 74\u001b[0m         score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score_batch(dataset\u001b[39m.\u001b[39;49mselect(batch), callbacks\u001b[39m=\u001b[39;49mgroup)\n\u001b[1;32m     75\u001b[0m         scores\u001b[39m.\u001b[39mextend(score)\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39madd_column(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/ragas/metrics/answer_relevance.py:83\u001b[0m, in \u001b[0;36mAnswerRelevancy._score_batch\u001b[0;34m(self, dataset, callbacks, callback_group_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m     human_prompt \u001b[39m=\u001b[39m QUESTION_GEN\u001b[39m.\u001b[39mformat(answer\u001b[39m=\u001b[39mans)\n\u001b[1;32m     81\u001b[0m     prompts\u001b[39m.\u001b[39mappend(ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages([human_prompt]))\n\u001b[0;32m---> 83\u001b[0m results \u001b[39m=\u001b[39m generate(\n\u001b[1;32m     84\u001b[0m     prompts,\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm,\n\u001b[1;32m     86\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrictness,\n\u001b[1;32m     87\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature,\n\u001b[1;32m     88\u001b[0m     callbacks\u001b[39m=\u001b[39;49mbatch_group,\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m results \u001b[39m=\u001b[39m [[i\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m r] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mgenerations]\n\u001b[1;32m     92\u001b[0m scores \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/ragas/metrics/llms.py:60\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompts, llm, n, temperature, callbacks)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(llm, BaseChatModel):\n\u001b[1;32m     59\u001b[0m     ps \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mformat_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m---> 60\u001b[0m     result \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mgenerate(ps, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m multiple_completion_supported(llm) \u001b[39mand\u001b[39;00m n_swapped:\n\u001b[1;32m     63\u001b[0m     llm \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mcast(MultipleCompletionSupportedLLM, llm)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/base.py:359\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    358\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 359\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    362\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    363\u001b[0m ]\n\u001b[1;32m    364\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 349\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    350\u001b[0m                 m,\n\u001b[1;32m    351\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    352\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    353\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m         )\n\u001b[1;32m    356\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/base.py:501\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    498\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    502\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    346\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/langchain/chat_models/openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/llmops/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[1;32m    620\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[1;32m    621\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    622\u001b[0m util\u001b[39m.\u001b[39mlog_debug(\n\u001b[1;32m    623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOpenAI API response\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    624\u001b[0m     path\u001b[39m=\u001b[39mabs_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     request_id\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Request-Id\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m \u001b[39m# Don't read the whole stream for debug logging unless necessary.\u001b[39;00m\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x2d0816ef0>: Failed to resolve 'api.openai.com' ([Errno 8] nodename nor servname provided, or not known)\"))"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        harmfulness\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
