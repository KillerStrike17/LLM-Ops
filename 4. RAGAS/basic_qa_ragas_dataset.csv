question,answer,contexts,ground_truths
What are the advantages of retrieval-augmented text generation compared to conventional generation models?,"Retrieval-augmented text generation has several advantages compared to conventional generation models:

1) Scalability: Retrieval-augmented text generation allows for explicit acquisition of knowledge in a plug-and-play manner, rather than implicitly storing it in model parameters. This leads to great scalability, as the knowledge can be easily updated or replaced without retraining the entire model.

2) Improved Text Generation: Instead of generating text from scratch, retrieval-augmented text generation generates text from retrieved human-written references. This approach potentially alleviates the difficulty of text generation and can lead to better quality outputs.

3) State-of-the-art Performance: Retrieval-augmented text generation has achieved state-of-the-art performance in many natural language processing (NLP) tasks. It has shown remarkable results in tasks such as dialogue response generation, machine translation, and text summarization.

Overall, retrieval-augmented text generation combines the strengths of deep learning technology and traditional retrieval technology, resulting in improved performance and scalability compared to conventional generation models.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it'
 'generation, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-'
 'itly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 'of multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) pro-\npose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25×\nfewer parameters.\nSummarization\nText summarization is another\nresearch\narea\nthat\nbeneﬁts\nfrom\nretrieval-\naugmented text generation.\nPeng et al. (2019)\npropose an adaptive decoding framework which\nﬁrst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline.\nSpeciﬁcally, before generating the\ndocument summary, the retrieval documents are\nﬁrst re-ranked based on their similarity scores']",['What are the advantages of retrieval-augmented text generation compared to conventional generation models?']
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"The advantages of retrieval-augmented text generation compared to generation-based models are:

1) Scalability: Retrieval-augmented text generation allows for explicit acquisition of knowledge in a plug-and-play manner, rather than implicitly storing it in model parameters. This leads to greater scalability.

2) Efficiency: Instead of generating text from scratch, retrieval-augmented text generation generates text from retrieved multi-lingual texts or relevant documents. This can save computational resources and time.

3) State-of-the-art performance: Retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks. It has shown remarkable results in tasks such as dialogue response generation, machine translation, and text summarization.

4) Improved diversity and control: By using customized metrics for retrieval, retrieval-augmented text generation can enable more controlled text generation. This can be beneficial for tasks that require specific terminologies, emotions, styles, or personalized dialogue generation.

Overall, retrieval-augmented text generation combines the strengths of deep learning technology and traditional retrieval technology, offering advantages in scalability, efficiency, performance, and control.","['generation, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it'
 'of multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) pro-\npose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25×\nfewer parameters.\nSummarization\nText summarization is another\nresearch\narea\nthat\nbeneﬁts\nfrom\nretrieval-\naugmented text generation.\nPeng et al. (2019)\npropose an adaptive decoding framework which\nﬁrst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline.\nSpeciﬁcally, before generating the\ndocument summary, the retrieval documents are\nﬁrst re-ranked based on their similarity scores'
 'with relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrieval-\nbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speech-\nto-text transcription could potentially beneﬁt from\nretrieval-based generation methods as well.\nDiverse & Controllable Retrieval\nMost of the\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneﬁcial for\nmore controlled text generation. For example, in-\nstances with emotions and styles may be more de-\nsirable in the personalized dialogue generation, par-\nallel data that contains speciﬁc terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval re-']",['What are the advantages of retrieval-augmented text generation compared to generation-based models?']
What are the three key components of retrieval-augmented generation?,"The three key components of retrieval-augmented generation are:

1. Retrieval source: This component refers to the source of information that is used for retrieval. It can include training corpora, external data, unsupervised data, or any other relevant sources of information.

2. Retrieval metric: This component involves the selection of a metric or method to measure the relevance or similarity between the input sequence and the retrieved information. Different retrieval metrics can be used depending on the specific task and requirements.

3. Integration methods: This component focuses on how the retrieved information is integrated into the generation process. It includes techniques such as attention mechanisms, skeleton and templates, or any other methods that facilitate the incorporation of the retrieved information into the output sequence.","['In this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response'
 'retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-'
 'Active Retrieval Augmented Generation\nZhengbao Jiang1∗\nFrank F. Xu1∗\nLuyu Gao1∗\nZhiqing Sun1∗\nQian Liu2\nJane Dwivedi-Yu3\nYiming Yang1\nJamie Callan1\nGraham Neubig1\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab\n3Meta AI Research\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\nAbstract\nDespite the remarkable ability of large lan-\nguage models (LMs) to comprehend and gen-\nerate language, they have a tendency to hal-\nlucinate and create factually inaccurate out-\nput.\nAugmenting LMs by retrieving infor-\nmation from external knowledge resources\nis one promising solution.\nMost existing\nretrieval-augmented LMs employ a retrieve-\nand-generate setup that only retrieves informa-\ntion once based on the input.\nThis is lim-\niting, however, in more general scenarios in-\nvolving generation of long texts, where con-\ntinually gathering information throughout the\ngeneration process is essential.\nThere have\nbeen some past efforts to retrieve informa-'
 'itly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has']",['What are the three key components of retrieval-augmented generation?']
What are some notable approaches to retrieval-augmented text generation in the context of dialogue response generation?,"Some notable approaches to retrieval-augmented text generation in the context of dialogue response generation include exemplar/template retrieval as an intermediate step, which has been shown to be beneficial for informative response generation. Other approaches include exploring different forms of knowledge such as knowledge bases and external documents to enhance the generation process.","['generation, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-'
 'itly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it'
 'retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-']",['What are some notable approaches to retrieval-augmented text generation in the context of dialogue response generation?']
What are the major components of the retrieval-augmented generation paradigm?,"The major components of the retrieval-augmented generation paradigm are:

1. Retrieval Source: This component refers to the source of information that is used for retrieval. It can include various types of data, such as training corpora, external data, unsupervised data, or task-specific retrieval sources.

2. Retrieval Metric: This component involves the selection of a metric or method to measure the relevance or similarity between the input sequence and the retrieved information. It can include sparse-vector retrieval, dense-vector retrieval, or task-specific retrieval metrics.

3. Generation Model: This component is responsible for generating the output sequence based on the retrieved information. It can include various models, such as machine translation models, dialogue generation models, or other task-specific generation models.

These three components work together in the retrieval-augmented generation paradigm to enhance the text generation process by incorporating relevant information from the retrieval source.","['In this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response'
 'itly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 'retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it']","['The major components of the retrieval-augmented generation paradigm are the integration of translation memory to NMT models, the applications in various generation tasks such as abstractive summarization, code generation, paraphrase, and knowledge-intensive generation, and the formulation and discussion of the retrieval-augmented text generation paradigm.']"
What are the three major components of the retrieval-augmented generation paradigm?,"The three major components of the retrieval-augmented generation paradigm are:

1. Retrieval sources: These are the sources from which relevant information is retrieved to assist in the text generation process. Examples of retrieval sources include dialogue history, knowledge bases, external documents, and translation memory.

2. Retrieval metrics: These metrics are used to measure the relevance or similarity between the retrieved information and the input sequence. They help determine which retrieved information should be used in the generation process. Examples of retrieval metrics include sparse-vector retrieval, dense-vector retrieval, and task-specific retrieval.

3. Generation models: These models generate the output sequence based on the input sequence and the retrieved information. They can be designed to incorporate the retrieved information in various ways, such as using attention mechanisms, skeleton and templates, or other techniques.","['In this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response'
 'itly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 'retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it']","['The three major components of the retrieval-augmented generation paradigm are sparse-vector retrieval, dense-vector retrieval, and task-specific retrieval.']"
What is the main idea behind retrieval-augmented generation?,"The main idea behind retrieval-augmented generation is to enhance language models by incorporating information retrieved from external knowledge resources. This approach aims to address the issue of language models generating factually inaccurate or hallucinated output. By retrieving relevant information during the generation process, retrieval-augmented models can improve the accuracy and informativeness of the generated text. This approach has shown promising results in various NLP tasks such as dialogue response generation and machine translation.","['retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-'
 'Active Retrieval Augmented Generation\nZhengbao Jiang1∗\nFrank F. Xu1∗\nLuyu Gao1∗\nZhiqing Sun1∗\nQian Liu2\nJane Dwivedi-Yu3\nYiming Yang1\nJamie Callan1\nGraham Neubig1\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab\n3Meta AI Research\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\nAbstract\nDespite the remarkable ability of large lan-\nguage models (LMs) to comprehend and gen-\nerate language, they have a tendency to hal-\nlucinate and create factually inaccurate out-\nput.\nAugmenting LMs by retrieving infor-\nmation from external knowledge resources\nis one promising solution.\nMost existing\nretrieval-augmented LMs employ a retrieve-\nand-generate setup that only retrieves informa-\ntion once based on the input.\nThis is lim-\niting, however, in more general scenarios in-\nvolving generation of long texts, where con-\ntinually gathering information throughout the\ngeneration process is essential.\nThere have\nbeen some past efforts to retrieve informa-'
 'generation, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it']",['What is the main idea behind retrieval-augmented generation?']
What are the three categories of metrics that evaluate the relevance between text?,"The three categories of metrics that evaluate the relevance between text are:

1. Lexical Metrics: These metrics focus on the lexical similarities between texts, such as word overlap, n-gram matching, or cosine similarity of word embeddings.

2. Semantic Metrics: These metrics aim to capture the semantic similarity between texts by considering the meaning of words and their relationships. Examples include word embeddings, semantic textual similarity models, or knowledge graph-based methods.

3. Contextual Metrics: These metrics take into account the context in which the texts appear, considering the surrounding words or sentences. Contextual metrics can include methods like contextual embeddings, attention mechanisms, or language models.","['more helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval re-\nsults. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for re-\ntrieval may lead to generation with higher quality\nin the future.\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences'
 'with relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrieval-\nbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speech-\nto-text transcription could potentially beneﬁt from\nretrieval-based generation methods as well.\nDiverse & Controllable Retrieval\nMost of the\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneﬁcial for\nmore controlled text generation. For example, in-\nstances with emotions and styles may be more de-\nsirable in the personalized dialogue generation, par-\nallel data that contains speciﬁc terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval re-'
 'Development in Information Retrieval, pages 3373–\n3374.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nVictor Lavrenko and W Bruce Croft. 2017. Relevance-\nbased language models. In ACM SIGIR Forum, vol-\nume 51, pages 260–267. ACM New York, NY, USA.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-'
 'SciFact\n\x13\nBinary\n920\n—-\n300\n5,183\n1.1\n12.37\n213.63\nTable 8: Statistics of the augmenting corpora.\nDatasets\nCorpus Size\nAvg. Doc Length\nMS MARCO\n502,939\n56.0\nMeSH\n32,326\n16.8\nWiki\n21,015,324\n100.0\nfor each passage as additional training data for the target\ndomain to continue to ﬁne-tune the TAS-B (Hofstätter\net al., 2021) model.\nLexical Retrieval\nLexical retrieval is a score func-\ntion for token matching calculated between two\nhigh-dimensional sparse vectors with token weights.\nBM25 (Robertson et al., 2009) is the most commonly\nused lexical retrieval function. We use the BM25 results\nreported in Thakur et al. (2021b) for comparison.\nLate Interaction\nWe also consider a late interac-\ntion baseline, namely ColBERT (Khattab and Zaharia,\n2020b). The model computes multiple contextualized\nembeddings for each token of queries and documents,\nand then uses a maximum similarity function to retrieve\nrelevant documents. This type of matching requires sig-']",['What are the three categories of metrics that evaluate the relevance between text?']
What is the main motivation behind storing knowledge in an explicit and accessible form in retrieval-augmented text generation?,"The main motivation behind storing knowledge in an explicit and accessible form in retrieval-augmented text generation is to improve the scalability and performance of text generation models. By explicitly acquiring knowledge and storing it separately from the model parameters, retrieval-augmented models can easily incorporate and utilize external information during the generation process. This allows the models to generate more informed and contextually relevant responses or translations. Additionally, storing knowledge in an explicit form enables easy integration of different types of knowledge sources, such as knowledge bases or external documents, further enhancing the generation capabilities of the models.","['In this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response'
 'retrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-'
 'generation, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-'
 'A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it']","[""The main motivation behind storing knowledge in an explicit and accessible form in retrieval-augmented text generation is to reduce model's uncertainty in generation and allow the model to re-access the knowledge during inference.""]"
What is the main idea behind the cross-lingual retriever proposed by Cai et al. (2021)?,"The main idea behind the cross-lingual retriever proposed by Cai et al. (2021) is to align source-side sentences and their corresponding target-side translations in a dense vector space. This alignment allows the retriever to directly connect the source-side input with the target-side translations, enabling the use of monolingual data in the target language as memories. In other words, the retriever can retrieve the most similar target sentence in a monolingual dataset using a source sentence as a query.","['ferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\nquery.\nLimitations\nIn the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be beneﬁcial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efﬁciency but may\nalso dampen the ﬂuency of generated results.\n5'
 'This is especially beneﬁcial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics'
 'sociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n874–880. Association for Computational Linguis-\ntics.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022.\nFew-shot learning with\nretrieval augmented language models.\nCoRR,\nabs/2208.03299.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know When language\nmodels know? on the calibration of language mod-\nels for question answering. Trans. Assoc. Comput.\nLinguistics, 9:962–977.\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022.\nRetrieval as attention: End-to-end learning\nof retrieval and reading within a single transformer.\nCoRR, abs/2212.02027.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020.\nHow can we know what language\nmodels know.\nTrans. Assoc. Comput. Linguistics,'
 '(2019) propose to ﬁrst extract a sentential exem-\nplar that serves as the syntax template. A neural\nmodel then generates the paraphrase with desired\nlinguistic syntax following the retrieved exemplar.\nText Style Transfer\nTo improve the quality of\ngenerated text, Li et al. (2018) propose a retrieval-\naugmented framework which ﬁrst retrieves texts\nthat are similar to the input based on lexical-level\nsimilarity. Then, the retrieved tokens that are irrel-\nevant to the source are deleted, and the output is\nderived from the edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating re-\ntrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.']","['The main idea behind the cross-lingual retriever proposed by Cai et al. (2021) is aligning source-side sentences and the corresponding target-side translations in a dense vector space, enabling monolingual data in the target language to be used alone as memories.']"
